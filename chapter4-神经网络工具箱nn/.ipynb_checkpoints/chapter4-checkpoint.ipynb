{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四章 神经网络工具箱nn\n",
    "上一章中提到，使用autograd可实现深度学习模型，但其抽象程度较低，如果用其来实现深度学习模型，则需要编写的代码量极大。在这种情况下，torch.nn应运而生，其是**专门为深度学习而设计的模块**。torch.nn的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是**继承`nn.Module`**，撰写自己的网络/层。下面先来看看如何用nn.Module实现自己的全连接层。\n",
    "\n",
    "全连接层，又名仿射层，输出$\\textbf{y}$和输入$\\textbf{x}$满足$\\textbf{y=Wx+b}$，$\\textbf{W}$和$\\textbf{b}$是可学习的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # 继承nn.Module\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # # 下式等价于 nn.Module.__init__(self) :调用nn.Module的构造函数\n",
    "        super(Linear, self).__init__() # 等价于nn.Module.__init__(self)\n",
    "        self.w = nn.Parameter(t.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # x.@(self.w) y = xW + b\n",
    "        return x + self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0672, -2.8913, -2.9386],\n",
       "        [ 0.8782,  1.5459,  1.5369]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Linear(4, 3)\n",
    "input = t.randn(2,4) # input = V(t.randn(2,4))\n",
    "output = layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-1.4743,  0.8361, -0.7465],\n",
      "        [ 0.7877,  0.3431, -0.9662],\n",
      "        [-0.1428, -1.0723, -1.8527],\n",
      "        [ 1.4845, -0.4600, -0.7687]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([0.2460, 0.7164, 0.9766], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in layer.named_parameters():\n",
    "    print(name, parameter) # w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，全连接层的实现非常简单，其代码量不超过10行，但需注意以下几点：\n",
    "- **继承`nn.Module`:** 自定义层`Linear`必须继承`nn.Module`，并且在其构造函数中需调用`nn.Module`的构造函数，即`super(Linear, self).__init__()` 或`nn.Module.__init__(self)`，推荐使用第一种用法，尽管第二种写法更直观。\n",
    "- **定义可学习的参数:** 在构造函数`__init__`中必须自己定义可学习的参数，并**封装成`Parameter`**，如在本例中我们把`w`和`b`封装成`parameter`。`parameter`是一种特殊的`Variable`，但其**默认需要求导（requires_grad = True）**，感兴趣的读者可以通过`nn.Parameter??`，查看`Parameter`类的源代码。\n",
    "- `forward`函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。\n",
    "- 无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这点比Function简单许多。\n",
    "- 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于`layers.__call__(input)`，在`__call__`函数中，主要调用的是 `layer.forward(x)`，另外还对钩子做了一些处理。所以在实际使用中应尽量使用`layer(x)`而不是使用`layer.forward(x)`，关于钩子技术将在下文讲解。\n",
    "- `Module`中的**可学习参数**可以通过**`named_parameters()`**或者**`parameters()`**返回迭代器，前者会给每个parameter都附上名字，使其更具有辨识度。\n",
    "\n",
    "可见利用Module实现的全连接层，比利用`Function`实现的更为简单，因其不再需要写反向传播函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module能够自动检测到自己的`Parameter`，并将其作为学习参数。除了`parameter`之外，Module还包含子`Module`，主Module能够递归查找子`Module`中的`parameter`。下面再来看看稍微复杂一点的网络，多层感知机MLP?。\n",
    "\n",
    "多层感知机的网络结构如图4-1所示，它由两个全连接层组成，采用$sigmoid$函数作为激活函数，图中没有画出。\n",
    "![图4-1；多层感知机](imgs/multi_perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义多层感知机中，使用前面自定义的 Linear层，作为当前module对象的一个 「子module」\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        #nn.Module.__init__(self)\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.layer1 = Linear(in_features, hidden_features) # 此处的Linear是前面自定义的全连接层\n",
    "        self.layer2 = Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = t.sigmoid(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.w torch.Size([3, 4])\n",
      "layer1.b torch.Size([4])\n",
      "layer2.w torch.Size([4, 1])\n",
      "layer2.b torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(3,4,1)\n",
    "for name, param in perceptron.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，即使是稍复杂的多层感知机，其实现依旧很简单。这里新增两个知识点：\n",
    "\n",
    "- 构造函数`__init__`中，可利用前面自定义的Linear层(module)，作为当前module对象的一个子module，它的可学习参数，也会成为当前module的可学习参数。\n",
    "- 在前向传播函数中，我们**有意识地将输出变量都命名成`x`**，是为了能让Python回收一些中间层的输出，从而节省内存。但并不是所有都会被回收，有些variable虽然名字被覆盖，但其在反向传播仍需要用到，此时Python的内存回收模块将通过检查引用计数，不会回收这一部分内存。\n",
    "\n",
    "module中parameter的命名规范：\n",
    "- 对于类似`self.param_name = nn.Parameter(t.randn(3, 4))`，命名为`param_name`\n",
    "- 对于子Module中的parameter，会其名字之前加上当前Module的名字。如对于`self.sub_module = SubModel()`，SubModel中有个parameter的名字叫做param_name，那么二者拼接而成的parameter name 就是`sub_module.param_name`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为方便用户使用，PyTorch实现了神经网络中绝大多数的layer，这些layer都继承于nn.Module，封装了可学习参数`parameter`，并实现了forward函数，且很多都专门针对GPU运算进行了CuDNN优化，其速度和性能都十分优异。本书不准备对nn.Module中的所有层进行详细介绍，具体内容读者可参照官方文档[^1]或在IPython/Jupyter中使用nn.layer?来查看。阅读文档时应主要关注以下几点：\n",
    "\n",
    "- 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注这三个参数的作用。\n",
    "- 属性，可学习参数，子module。如nn.Linear中有`weight`和`bias`两个可学习参数，不包含子module。\n",
    "- 输入输出的形状，如nn.linear的输入形状是(N, input_features)，输出为(N，output_features)，N是batch_size。\n",
    "\n",
    "这些自定义layer对输入形状都有假设：输入的不是单个数据，而是一个batch。若想输入一个数据，则必须调用`unsqueeze(0)`函数将数据伪装成batch_size=1的batch\n",
    "\n",
    "[^1]: http://pytorch.org/docs/nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将从应用层面出发，对一些常用的layer做简单介绍，更详细的用法请查看文档，这里只作概览参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 常用神经网络层\n",
    "#### 4.1.1 图像相关层\n",
    "\n",
    "图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维(1D)、二维(2D)、三维（3D），池化方式又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。而卷积层除了常用的前向卷积之外，还有逆卷积（TransposeConv）。下面举例说明一些基础的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宽：200,高：200\n",
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "to_tensor = ToTensor() # img -> tensor\n",
    "to_pil = ToPILImage()\n",
    "lena = Image.open('imgs/lena.png')\n",
    "print('宽：%d,高：%d'%(lena.size[0],lena.size[1]))\n",
    "print(lena.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.type() is :  torch.FloatTensor\n",
      "to_tensor(lena).shape =  torch.Size([1, 200, 200])\n",
      "kernel =  tensor([[-0.1111, -0.1111, -0.1111],\n",
      "        [-0.1111, -0.1111, -0.1111],\n",
      "        [-0.1111, -0.1111, -0.1111]])\n",
      "kernel.shape =  torch.Size([3, 3])\n",
      "conv.shape= torch.Size([1, 1, 3, 3])\n",
      "conv.weight.type()  torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=198x198 at 0x7FC9037CFA90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入是一个batch，batch_size＝1\n",
    "input = to_tensor(lena).unsqueeze(0) \n",
    "print('input.type() is : ', input.type())\n",
    "print('to_tensor(lena).shape = ', to_tensor(lena).shape)\n",
    "\n",
    "# 锐化卷积核\n",
    "kernel = t.ones(3, 3)/-9.\n",
    "print('kernel = ',kernel)\n",
    "print('kernel.shape = ',kernel.shape)\n",
    "kernel[1][1] = 1\n",
    "conv = nn.Conv2d(1, 1, (3, 3), 1, bias=False) # 卷积层输入为：nSamples x nChannels x Height x Width\n",
    "print('conv.shape=', conv.weight.shape)\n",
    "conv.weight.data = kernel.view(1, 1, 3, 3) # 重要 更改维度\n",
    "print('conv.weight.type() ', conv.weight.type())\n",
    "\n",
    "#out = conv(V(input))\n",
    "out = conv(input)\n",
    "to_pil(out.data.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上述的使用，图像的卷积操作还有各种变体，具体可以参照此处**动图[^2]**介绍。\n",
    "[^2]: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AvgPool2d(kernel_size=2, stride=2, padding=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.AvgPool2d(2,2)\n",
    "print(pool)\n",
    "list(pool.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAAZt0lEQVR4nEV6Sa9lWXbW96299znnNq9/8eK96DIiMiOzsquiqih32FbJNjAwMp5YDJAQA5gwYMIY8Q8QI4QsMfCkQJjGSBjbYKrswmVX48rKqsiK7CIjMzKjyXgRr73NafZeazG4YXMHV1dXV3efs9c63/qazd+hmrnUlfRPPmyb6KSHOkYJhCGAhIkL3IsQEPeQaTQ6SumGCzvj0FFE4UIyvT6xb789rkg4hRpd8952LDQ3GAgdvBQxguJ0E4BuQndXKpywoIRR3d0AOBzeZQ3B4AQc8O68CdfuqIoQhNEZ5eP/LTCQjMFVzeEW6Eo3pxlIVwNAd4cTBvD58iwghXG+zB5iTFFIV9UHHa69nIu7uxkIxsd/+UTcCUgFLYV0Ak44ACfhpACkgySFBBhIgrBECCXYIpeYggQJIBznh1p9eVrUGSgMVfrwRz3EhUIR10IhXGy1isEdbu5mZg5XLcXVzZ5/62YwBuNQ3Al3GAE6Hsx994tubu4IKXx4xxBFQISKXiBW1I0AHCDgZgDc3DzQclZTs1KccCfdQRdhKPZ8a0gRon2cw+u7xYXu8Hd+ko0iJKUKdAfIQDzfLJiv3kAXV4O7GszcHauqA4CCsFLMnQ6nUISfner6L4ScPQR5/x0PiRARiWJFCTMtjmIA4OrmMHNzBWHmAlgpZv5X9wkDGGmmpZibwwnAkD8ruHazL8rw7k8s1cQgAGPuB2B1eU7CHSThBqGbwBxOCVKPa+SianCKgAZzumkp6mYGdS1FcXhk1VeanP3Ddy1Fc/VID9qWMHGKCAxiXD2CIEgQEHNvQorjtRh9dvy0j4xiBqdT3RwFoBsBteKGcn9jevFvvJU+umMhktklepB2kddIAOZugAACQFzECHf1OGlymFZMqpsXvnD0wcID4W5mcJA0JWEGV1PQnz66WX/l8Tt3LET0Jm7CShfHbRSSsOKqVuhm7mqmbjpaH6nYktL3Q6uBiFd++WbOpZhbMTUA4l7MSynmDnfaRwvbvHG7MImbEEEqdiePlSRI6qBqMFMzdbhbiuXs3HVou0E9u9TBLcc3fyEscimrX0KEbsVZ1AARhtDd0/ydXhKZDYUiKc8fn8FBqJqRgTA3e35VcrToYVB61/aMLE4Cvv1r1+bzvpg5nRKCl76jAAQgkOrp4Te+FYUoknJqTqO2x08DSHeHggI6VkAlxrXT0EWJFqPTy9COEqMIneFn198/HkNpihDE2zZILMEBoVDCf/y2S+2DN7kaH31JutkjE7oLV93kTkJWkMXSZqHCtMDc1VOguufM8cbX/+nmbMjFCCejHz+ZQ8xJAowP/0IZvViTsT5c+jVZPjkVAQCYklSBrOCREHlmVQwciguAwd1d2Lb11lrQNvxGbrWompsztPeedgICAKT907lX0bUaOEm7X09y9FAC4e5cbakJnAYQkHkWUtUDA2NMpiUvziYHI1VV7a7++rwr5k7CQ3z62bnTAYD8/nlsAkrtNpqs/czmXB71CWbucDoYAszoJN0l5ySiRauYWNWelXaEa+MM6xXU/LWvLOeDEoCQ5elxK3CA8cHtIYp3Il7JxmtbeV2OAoTmK4A0dcIFDoCxje6Oqo6Mbjm7leP9XZXS5oCSfci/9Q+sVQdIQTi7f5IJY+z+LLMWQ9WH0Nzc9kktCGAQ+F/Du0AdTht3fQkwYxUnElBKWcqNDe3mAxJDCKq5fPXvzAZzdzcJ5eGDucMpbz2NyV3rPkJe3S+xgUTA1QG4rHrLhUJovroQFEqMSNq753m5cZ2tZaMIDKpa3L/+8lIdBBCku/upX6iqB7dDim7Bg7QvvqjV2CzC3N2wGigr8Aows9FahkQTiZUKbV62Xlpf1oOOaijEjBAUn/76vzWYQAh4++Wf1+B/lFOCGDUtrn3JwzoU0Shubi50z6pOhxmsvLB0klECS65k1t46sCVmo9oLkZkiPUez5a2rnQvdNeeL/+xa2Ujpq+8DyLSo06+O8oYoXEIIMThEBHC6iASBBLkyz0DxkPo26qx/Y7/vB29ywJB7iZ7V3IHBfrOYCLVdfulfbaXpMOteA0xtgPibl/KEBY4YGGzFSUIIIQkoCOK+cVe8b2LoMJbWX2nmIg206dmHZjV3hiqItFe//qmKLvt/9IvdXmpzTfEgnYcw339Nm8rcTWIkSMAhIhSQpAJo4jms9t7q2GfexJlMKg3ehflIvE2s7FlT04uWW3JY8sVfqO2CtRYTngZjzMHwlVFuHA5oJJwxEMJAQuA0V+S9ZaxtYGPWYrRjg05cBmvkdGreSq77oZEhGGTe7nxev7jeVql3CUJ8EoMVIN26lms6aO7RxUhIEAbCYeJG47CPJngIEi3KLpc6jn46TUuMVHqJaR7qXPmQgyvTjY3B15FdJAO85y4ews4blAR3AJRVC7sI3OkON7i7bVq1NSIy6Ttl6JswW45tHpKxYOj7yL4u/aIty/PMYJNmKArmZ6onH6k5U76+YzWdpDBEA9zwHCJDoBgd7rGMB+37SZF9xfl2MJS5r4k6tIvdEOrY21APZ9wWroUBdAl8tMy7d5ZJMmz/utVhxYDEBWYGEZiDWJE0QEuwECduJewN+WQqfWZYxq6YLdT6bFh2XTssumYD44m2mUIJZ3P73L5lUSHTL048Pcd2IAJwrgaKCAADYdARYzVMtd4puthOfW7CMM1h2YdUMDIbMHeWZttHsYNIiEDEY7RVfj+pRN+7bBVdIeZ0RAQLFR0EaWpOC07VRkbZh/PNnu2W52UK3drJPI/RPHh51Fr081TZfkgj7z2JRELi4/nH7a+8PZ+6hHAzWsKqwHCPhAiFpK9IIyMImFW1+2i6qBfbyF0Vl9PTpcw+ib8aP+05OZ63p7+1m9eawoAQIE4++cH78rP7/7pyrcrBQRmJwelwh0cGZRChr+iiwwkynI/XHh55jic7krtKuumJ8lTH7U/LXYyPj5b1L1bVtmW6B7rTP/7Jw3byxmv/+agi2L8YWIEQd9AdMdCxamenkwBocD68ao7JcL7R5LZiHs365mg4PGIt69XD2KW/d2F7VFYFZAjPvvlA1196+cq/eye51rhwYGti9NUyQIwWXOFGBwS++hCqD3/OpmepH0+GRc1uzeejD878tH01/vhKvxxP/v7ajaqrYSJCefb9ki7v7k42f+entRQP/kqDaiWnnE5oJFalAAA+ZxMu6XAZd8oT27TjqffVUSg/HvJZmeCnlzbvLa//avMF9KF3VuDwR+9eDa9MQ9r43e/VwS3m8bUyJVasAnSnQOCqIEnC3GHmCCn95Thvnm2U0zXkCt2dDy2fH7TbH9ni2YWtV5o3ksEHNfHhm2+txXRqo/jb366iwMkrwRvQSQGFhAtAz8VW9JPqBCkMox893YqjPNSBkhfHqbYn07K2QN2Wza/pFzBkU42hWP3iVjqrS/Po39xJDX1gGF9aVgG2Qi2u/s8dRd2dpJg7aABjCnfXGilLWXgoTx4/Y197XR/1i2U6Tde75bzVIrlpmnjl4OH5o/Tw956wkugabHvDR6u5T4qDQDTQzZ9rLAmBhIOCVJrj4zSe7/jJe6VdxGo9VzvLvLy+d+HF7TQsK5lN1mu6VLu4f0VtISKIA51XpYGt1K07CK56YIUqTllpGglgqI54Nw9lenR4cshzOVPWn+6Nlld/7o0X1NlanFejSKma6e54ftqzDQHimb61z/V+MSjcV8V3xOdiF6S4CBnopru5VCcnfZObMHt8JKVJ/XG8VC8PL/7yzv6isHg8aeokZJDqwkY8fnRx1IYQe1T5YkrNoB2CiAhgDkSaqa/AHohR3MnQeUS5vTHb5OzzuS+3BZK6Nuver0z2xv6sPu+HuhIBhQjrm+OzY6laETdBvOmTyGzmvTMKHHBRd/pKgtLMAQqliJZ499laHD4405RO9vqSJs/KO39rcqFjM1qcaaoDFKrqmOxs+LknSvAcyuUpa0McjaoE7xftYCvShRXZptMNgMGJOLl4Wnl3No8NuT0bwrDWnH0p3BgVVuGHRzmmaEPRUooitT4fNkYSMoVXvKK7e4ixChTPXTcIKEJ3CCBOPrc9KHuPv/Pu1c/e3z6/1PGsjzb1buP6a+MU0tGH8vhP/uTjbH3RIfcP/u+dTJ1tmlgBmt0ycTXN2Uxi3VT0vIgQFDOHUSCkUJ3McmBbX2refhz7gwcb7eLq/TIft796c7r2+e6d++P5fp795fx6rdFP3/l4GO31pVsrSU2420h0d3cXuAvpVjxCXYsDwkJRNQNM93clxpe+e7Z76eHupxfq46MXP5odf/XC1pqsf+etrhvGy1dv2dl0Mw0//uGztokqQ53YO7p9GeGvjB+H0dSDSclD3/cuAMyMrqp99Utus/vfPLyK97Yjzprp6fyirL10TRr84M8fd+36P3zj3sz7IpPv/Pfjr+0vFy6tjFjo9UWZrNwrN4eV0ucua5Tn0oESwBiTQIZP3n6h+7OPFtcfjXzcIT/bap9mvLGVN+Wdp5vdK4drr974b7mtMT7/5JLvlwcF9VBPjyHlwlpY2XzidDOzknNYjwLj6mGkhCqm6B/Wr+1t/8F7wxufhcmmbI/bpSYfbly7tDF6+7MLVz6uD26V+Y1ZKRg/2j4Y7d0elVK1o1BUeJE1zCzEImZqxYpc3JxGYS6ucEpY0fl682+W9/7Hh5cO8mwzdKN0cFeyZHn9cpr88N5eHV/uZw8fHGwuo+ey+dLR/NvnFx+59BueEdIVrc0d5q4oWhTbu5OIqHnQbjAAJM1M9JfO3rLDapHSeFZdPmtKVMDeXN/bvPPWpTSOLFuxlqFin/Noq9CnZ4dZS6UObcaB7kYoYGZFru5FN8S+5FzM3Vdz3sLoWG6+55f2lt2z6Xa7OU6jZZHJrUtrh7//QnBUVaXitqwzhzytm36MgRFt0sKyLw1tZd+alcIbF2N2SBxU1QCsTFYbb6TZu/l8I47C+6n7+ODz+38Xd5nfbK7w309qumkI0X1gMil91CBMwYJ0ZZyTHkizwlt392JXL4iTIcZetbj+tVfZmS42h5g63svR20fBhhvvaX312vQ/Pd0W0TIMManmYq6pD/Qg6NRkOWx5hc2qHoDVvmjZ2A9GSZFSDJodZmqOsp4nm0PTcNmnUZ1KHpW1gK+Ul3nzs2+NVAHzoR9US26z6mI1r+dDRFc2abtNWk0mN9MSrtTurBpxEYFpWTE903AQn85jN9u/FLaK9WF/49YXPjrxm5ftt3Ws5qamXTsUy6VY7svI3MupV8j9BP1+qNQcQsLNdjeBMKqEjKLQXIwUF+IsxYHzfmqdLwvH0r4of3i//xr3/svjagoz10KziL5vp46+1AO0PWIwnU/Hw34IK7YAQsNBUtbRXCgxIIRIOMiwvbnQszBZhlMs0uVY9/bO4s78hd945fjtIDUFBjM3Vc8hS2kXCyvl81agPK+m9Vak+8pNdlufOFMwBynFJEw3K8DBcH5oGaJnBz7YxY1uI9298nloXhvv/oejxLpxw8pYRxjz84fHz54eQ/P9EIKHZ5hcraK5O9yKuW9GxIqMgYju4Np6glLgEFRnk/eufpImJV18e/PmN+flZ1548uQHT3YW3nSBZhqc4jJhNfKXmlPPj+eMI4tHGO8h+mrw0SxMIUkcdGdUh3otDoEQLhvJbj57utPw2cEtf7LV/e3LH8jZW2Ote4+BJIskA9jUV2VjyFruiddgPOH0gothZbO7pwohAjQhpZRsNCcdhNr2mV98OpvY2cmhXpl+uvvqHu6v/25US6W3Qrq5dkOfi4zWpxGDHZ4giJt1w2QUVvGNO1BSkooQJ4HYe4IbVpGBXzgZD+0MMmUbPnj5k6u/sjf6X1e/W3aO2HgfBgvuMLQxINq9zT4fV+8jMCCFTseBgDudbu5BquCEiBhiF2hBJES66w4wzQ+YlmuT+fn2R9M3dl76r/X8ETon2MfOUChV+zFGacij9aPR2uFhyBPz3CyXG2klpUAjLKVqlWuBiO6G+rkkmsqnrx01dnxhujbH7lE8aGbv9y98o+4KWeoebe+BUo2vt5amu83avVK++dwVrTmsizgQFE53r+tgJEl3jwYHi7sANe5c70/X8tHENssz3cV+Xr/7xW+UlDTCUyFaqyhDrG3w7lQeI75/FCEamBCtdkDcxRVummpxhJXvLaqZKQ8GsJ/p9snTyaxqZ7vlVldtPpylV/7wSEiKqPS2OGv7Yrkg1qGcnDEevueeaktCRpUVewugmxVEShDkghCjq21Mq+JGMMoih3ZedViOm6tPriy4+cfvBY0O62NcSjmlJINJasIk5dLd7hliseRCKVUWN3cC7mpB4FATMXhUQXyeEsJ4NgwTicPoyafcGOzgo+99ly7RgQxK9uX5gaYoMU5iQpr1jxgTiLoUrW1VCwfdzDGVohBR9ZUV1fVR6CRR/HG3E8UWG90DuTisjX80CBgYNGgQZ5h/0mFtSg6JinyoqYYMIZTARNeOTndTc5UpDUItunz0bjTqIi/pRmh/dX+4fxJtUHnx0+Vivb5664+fhroyybTciOfdzk9N1WKLUnIfCDoE0cJQ94cYVSGKA6WMpmpQL7MP/uynOY5sWc7rAhjzC/hztjtDHreznc8/fFMe8s5v/v7DldUGL8G69ZsXmhiZxBZhfOmNr/zFu52WBgayXvbaUUJT0a3sVYPSyuz27x1Od+OLz8RmYaFuYfvsT6/9qC95uPn4J1++sHfw9nhjsnhz5oWRFrwkJ8ZBjVEHQzUNzauvPf7Wd/vkRel13kDfd+UkBpEexYRY3P6D053tKK+9+cJOPmvdGbsnr2CG8wuzyd7RvZTfG9fVXvfuGzY44JG0EQfNRnMUDXWSnO3yP/4X13KXc9YQJuPJxtak6s+O/eTYIDZ75/e6g61KRC6/ernJIFGGW5MPWJqtn+kX23gwPJg16xvv7Z9d7EMIZEJskjtoEsUQk5eSi9oX/uXrx+1gfZIU63q0vr65NlmvdkISe+cbabder1zacOHqhoNRiKvDrF6z248+XdgnG58EHHeyV23vDwSDSKimG22UXCVzpkjt+5yL2cY/v7woLoEEQ6iayfoat2+E4Cc/ibvV+hoauX02PthxUoLIk/vLbCpPLobDnXf3hiXbxSXIhmT3bOjL2lYuOg2gC91yybmYmq3/k9jO4io9EYYQm2pnR6D3n1ySeqOwkbuP02hzcILkbAJMFirrOc7Gi63dxdEQ5YdXNsoqEdOmztbQnUKaWlY3czO7+hv9slFx54pG+ORGQ+8+CKGZdotaZDjxSgZzgvHMY8zFZNo2C+SFTM9SOMVOTrES7zk2VpQiMMOKqKkXB81+bbqYiJCAmaluv3y9oj0+HFU1lpEmed7DzQgKtp5K3Wrejb1NZs/aeT337mDz8XZyDSqy1kyjhEjY6uXFDOpwXf/FYbpip2ZZ5fWDStB+kL2ue68B8aVKTSXgXM5GS+fowmOItnX7VDYD49cflLhkgK5dGpc6mAhBNxhUV6cC3PTnMQGeRyQ63kmRevgkMea+EluKF/WmUtgwFK63CKPRR1ZTg9R867o8u/tELl/ZmjplfSQ5peAUFjM1MyvqbmauN/Yah6k7xTkJg7P7RELCoFGHHNl1UgU3JobQimEQPQl90+a1tjmDjn94ht0tn7cbGzG0kxEDVla1azDVnMzEvf5iQ0AKAMZQEO34qdGHeQyd1ZL6VsPIzOHeNMhpV72lIAi26iiPN65vyfGDjwefjmGn0yoF+f/muxVdlUd/Pg4ZTCII1fLusejpcsg400lXAoWlLXFi5gb6JKTR8ZAYG60mZyW++8A+uHXNtM9DvTbivNuIpJDigLuWbJrNQPjr9SefHrUmkupp+Pgk5aO81DxMy0CcRw1DaoKaC2QuQealif30LGtMa1e/9/7W+r33XW2YbK6LP2w2CIJ0WRkC9JxKcAc3l59xNJ42TTNtTrHN88PT2fR0HYM058eRuQ+jaTEHuajJISVVbXKVy4CffX/r5MfI1DDamsrZJ19e+SYAXElQNRR1DQLuNjIsHyNMb1za7zbKxx8/XluMxi3q0vayfREyHtFhDPMyWBWHQvdaHffvb7+Gqh3ENK5vQG/vXgiEm4is4lq3XLwUNXV/Ja5vraXlw9vf276yWZ/d+RTFtvrBYp5bvInG0ubCKWIh5BS6yioVM4nxXvXK1INaqbY2Jrjtr1R4Hh2YuCghtCGYqSgOxmiE9JNh09e6H3zf6nJxOJdJyqkW3Zo6xoQLWZaiQwjW18MFGdfV+vFn3QMPLtPNtdn3+9enz8WaQ1Zqqrjr0Gd1eLrWeYjTzYOd5Zb9n//Z17YTTrWedpareLo960OgEc68LA0hEevtxixXEm3rrpbA0MyP/GB/ugrYAIqYKE3MSvBStAT3l36aYgrc5nD0p+/mTVyYHM0m69QZLJ5+jn40CgoRyUdBK0kopR9d/+kLH758PBw8jaD0zeXtSYo0BTU4QVAIh7karAQVv+a91FrX5Xb/sKm78ejkzHYm2g+5j+Gsks3dJyTQPwvihI6fpb7diqPp07WHEzWm7YOtKiXh6riB0whBMBe4uWaREIKtb8zAOoHzfjOcjMZHC9najOenfTEp7XJ77WrtbmaTPohIZfUkVecv3dteVJ/bFcXW/kZKFDcJukq8HCRDogQHTM2sQG7kkoMEidOdkCZ9CaPLo+XJaQl7Yrq0AiGI/txNQtXkF2e1HY5Hm/1MH9ubO9tVCBJAcSdWh8ugcEEg6aZqpup2qxmGITTV5ELs8snQVDsb/uzzBXYP/h8CwMEc4WwTQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100 at 0x7FC9048912E8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out = pool(V(input))\n",
    "out = pool(input)\n",
    "to_pil(out.data.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了卷积层和池化层，深度学习中还将常用到以下几个层：\n",
    "- Linear：全连接层。\n",
    "- BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。\n",
    "- Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。\n",
    "下面通过例子来说明它们的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6595, -1.0887,  0.7219, -0.5690],\n",
       "        [ 0.6917,  0.1715,  0.3225, -0.3611]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入 batch_size=2，维度3\n",
    "#input = V(t.randn(2, 3))\n",
    "input = t.randn(2, 3)\n",
    "linear = nn.Linear(3, 4)\n",
    "h = linear(input)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<MeanBackward0>),\n",
       " tensor([15.4065, 15.9996, 15.9960, 15.9852], grad_fn=<VarBackward1>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 channel，初始化标准差为4，均值为0\n",
    "bn = nn.BatchNorm1d(4)\n",
    "bn.weight.data = t.ones(4) * 4\n",
    "bn.bias.data = t.zeros(4)\n",
    "print(bn)\n",
    "\n",
    "bn_out = bn(h)\n",
    "# 注意输出的均值和方差\n",
    "# 方差是标准差的平方，计算无偏方差分母会减1\n",
    "# 使用unbiased=False 分母不减1\n",
    "bn_out.mean(0), bn_out.var(0, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8502, -0.0000,  7.9990, -0.0000],\n",
       "        [ 7.8502,  7.9999, -0.0000,  7.9963]], grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个元素以0.5的概率舍弃\n",
    "dropout = nn.Dropout(0.5)\n",
    "o = dropout(bn_out)\n",
    "o # 有一半左右的数变为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，应尽量不要直接修改这些参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 激活函数\n",
    "PyTorch实现了常见的激活函数，其具体的接口信息可参见官方文档[^3]，这些激活函数可作为独立的layer使用。这里将介绍最常用的激活函数ReLU，其数学表达式为：\n",
    "$$ReLU(x)=max(0,x)$$\n",
    "[^3]: http://pytorch.org/docs/nn.html#non-linear-activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6186, -0.3954, -0.7482],\n",
      "        [ 2.5139,  0.9241, -1.0873]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [2.5139, 0.9241, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU(inplace=True)\n",
    "input = V(t.randn(2, 3))\n",
    "print(input)\n",
    "output = relu(input)\n",
    "print(output) # 小于0的都被截断为0\n",
    "# 等价于input.clamp(min=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ReLU函数有个inplace参数，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确地知道自己在做什么，否则一般不要使用inplace操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上的例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络（feedforward neural network）。对于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1: Sequential(\n",
      "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation_layer): ReLU()\n",
      ")\n",
      "net2: Sequential(\n",
      "  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "net3: Sequential(\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Sequential的三种写法\n",
    "net1 = nn.Sequential()\n",
    "net1.add_module('conv', nn.Conv2d(3, 3, 3))\n",
    "net1.add_module('batchnorm', nn.BatchNorm2d(3))\n",
    "net1.add_module('activation_layer', nn.ReLU())\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "        nn.Conv2d(3, 3, 3),\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "from collections import OrderedDict\n",
    "net3= nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(3, 3, 3)),\n",
    "          ('bn1', nn.BatchNorm2d(3)),\n",
    "          ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "print('net1:', net1)\n",
    "print('net2:', net2)\n",
    "print('net3:', net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可根据名字或序号取出子module\n",
    "net1.conv, net2[0], net3.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = V(t.rand(1, 3, 4, 4))\n",
    "input = t.rand(1, 3, 4, 4)  # 卷积层输入为：nSample * nChannels * Height * Width\n",
    "output = net1(input)\n",
    "output = net2(input)\n",
    "output = net3(input)\n",
    "# conv -> batchnorm -> relu\n",
    "output = net3.relu1(net1.batchnorm(net1.conv(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellist = nn.ModuleList([nn.Linear(3,4), nn.ReLU(), nn.Linear(4,2)])\n",
    "#input = V(t.randn(1, 3))\n",
    "input = t.randn(1, 3)\n",
    "for model in modellist:\n",
    "    input = model(input)\n",
    "# 下面会报错,因为modellist没有实现forward方法\n",
    "# output = modelist(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到这里，读者可能会问，为何不直接使用Python中自带的list，而非要多此一举呢？这是因为`ModuleList`是`Module`的子类，当在`Module`中使用它的时候，就能自动识别为子module。\n",
    "\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.list = [nn.Linear(3, 4), nn.ReLU()] # 未被识别\n",
    "        self.module_list = nn.ModuleList([nn.Conv2d(3, 3, 3), nn.ReLU()])\n",
    "    def forward(self):\n",
    "        pass\n",
    "model = MyModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_list.0.weight torch.Size([3, 3, 3, 3])\n",
      "module_list.0.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，list中的子module并不能被主module所识别，而ModuleList中的**子module**能够被主module所识别。这意味着如果用list保存子module，将无法调整其参数，因其未加入到主module的参数中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除ModuleList之外还有ParameterList，其是一个可以包含多个parameter的类list对象。在实际应用中，使用方式与ModuleList类似。如果在构造函数`__init__`中用到list、tuple、dict等对象时，一定要思考是否应该用ModuleList或ParameterList代替。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 循环神经网络层(RNN)\n",
    "近些年随着深度学习和自然语言处理的结合加深，RNN的使用也越来越多，关于RNN的基础知识，推荐阅读colah的文章[^4]入门。PyTorch中实现了如今最常用的三种RNN：**RNN（vanilla RNN）**、**LSTM**和**GRU**。此外还有对应的三种RNNCell。\n",
    "\n",
    "`RNN`和`RNNCell层`的**区别**在于前者一次能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上RNN层的一种后端实现方式就是调用RNNCell来实现的。\n",
    "[^4]: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3610, -0.1643,  0.1631],\n",
       "         [-0.0613, -0.4937, -0.1642],\n",
       "         [ 0.5080, -0.4175,  0.2502]],\n",
       "\n",
       "        [[-0.0703, -0.0393, -0.0429],\n",
       "         [ 0.2085, -0.3005, -0.2686],\n",
       "         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(1000)\n",
    "# 输入：batch_size=3，序列长度都为2，序列中每个元素占4维\n",
    "input = V(t.randn(2, 3, 4))\n",
    "# lstm输入向量4维，隐藏元3，1层\n",
    "lstm = nn.LSTM(4, 3, 1)\n",
    "# 初始状态：1层，batch_size=3，3个隐藏元\n",
    "h0 = V(t.randn(1, 3, 3))\n",
    "c0 = V(t.randn(1, 3, 3))\n",
    "out, hn = lstm(input, (h0, c0))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3610, -0.1643,  0.1631],\n",
       "         [-0.0613, -0.4937, -0.1642],\n",
       "         [ 0.5080, -0.4175,  0.2502]],\n",
       "\n",
       "        [[-0.0703, -0.0393, -0.0429],\n",
       "         [ 0.2085, -0.3005, -0.2686],\n",
       "         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(1000)\n",
    "input = V(t.randn(2, 3, 4))\n",
    "# 一个LSTMCell对应的层数只能是一层\n",
    "lstm = nn.LSTMCell(4, 3)\n",
    "hx = V(t.randn(3, 3))\n",
    "cx = V(t.randn(3, 3))\n",
    "out = []\n",
    "for i_ in input:\n",
    "    hx, cx=lstm(i_, (hx, cx))\n",
    "    out.append(hx)\n",
    "t.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量在自然语言中应用十分普及，PyTorch同样提供了Embedding层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有4个词，每个词用5维的向量表示\n",
    "embedding = nn.Embedding(4, 5)\n",
    "# 可以用预训练好的词向量初始化embedding\n",
    "embedding.weight.data = t.arange(0,20).view(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 16, 17, 18, 19],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [ 5,  6,  7,  8,  9]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = V(t.arange(3, 0, -1)).long()\n",
    "output = embedding(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 损失函数\n",
    "在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可看作是一种特殊的layer，PyTorch也将这些损失函数实现为`nn.Module`的子类。然而在实际使用中通常将这些loss function专门提取出来，和主模型互相独立。详细的loss使用请参照文档[^5]，这里以分类中最常用的交叉熵损失CrossEntropyloss为例说明。\n",
    "[^5]: http://pytorch.org/docs/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5944)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size=3，计算对应每个类别的分数（只有两个类别）\n",
    "#score = V(t.randn(3, 2))\n",
    "score = t.randn(3, 2)   # 行和输入保持一致，代表输入了几个例子\n",
    "# 三个样本分别属于1，0，1类，label必须是LongTensor\n",
    "#label = V(t.Tensor([1, 0, 1])).long()\n",
    "label = t.Tensor([1, 0, 1]).long()\n",
    "\n",
    "# loss与普通的layer无差异\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(score, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 优化器\n",
    "\n",
    "PyTorch将深度学习中常用的优化方法全部封装在`torch.optim`中，其设计十分灵活，能够很方便的扩展成自定义的优化方法。\n",
    "\n",
    "所有的优化方法都是继承基类`optim.Optimizer`，并实现了自己的优化步骤。下面就以最基本的优化方法——随机梯度下降法（SGD）举例说明。这里需重点掌握：\n",
    "\n",
    "- 优化方法的基本使用方法\n",
    "- 如何对模型的不同部分设置不同的学习率\n",
    "- 如何调整学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先定义一个LeNet网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "                    nn.Conv2d(3, 6, 5),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2,2),\n",
    "                    nn.Conv2d(6, 16, 5),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=1)\n",
    "optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "\n",
    "#input = V(t.randn(1, 3, 32, 32))\n",
    "input = t.randn(1, 3, 32, 32)\n",
    "output = net(input)\n",
    "output.backward(output) # fake backward\n",
    "\n",
    "optimizer.step() # 执行优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为不同子网络设置不同的学习率，在finetune中经常用到\n",
    "# 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "optimizer =optim.SGD([\n",
    "                {'params': net.features.parameters()}, # 学习率为1e-5\n",
    "                {'params': net.classifier.parameters(), 'lr': 1e-2}\n",
    "            ], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只为两个全连接层设置较大的学习率，其余层的学习率较小\n",
    "special_layers = nn.ModuleList([net.classifier[0], net.classifier[3]])\n",
    "special_layers_params = list(map(id, special_layers.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in special_layers_params,\n",
    "                     net.parameters())\n",
    "\n",
    "optimizer = t.optim.SGD([\n",
    "            {'params': base_params},\n",
    "            {'params': special_layers.parameters(), 'lr': 0.01}\n",
    "        ], lr=0.001 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于如何调整学习率，主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整学习率，新建一个optimizer\n",
    "old_lr = 0.1\n",
    "optimizer =optim.SGD([\n",
    "                {'params': net.features.parameters()},\n",
    "                {'params': net.classifier.parameters(), 'lr': old_lr*0.1}\n",
    "            ], lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 nn.functional\n",
    "\n",
    "nn中还有一个很常用的模块：`nn.functional`，nn中的大多数layer，在`functional`中都有一个与之相对应的函数。`nn.functional`中的函数和`nn.Module`的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由`class layer(nn.Module)`定义，会自动提取可学习的参数。而`nn.functional`中的函数更像是纯函数，由`def function(input)`定义。下面举例说明functional的使用，并指出二者的不同之处。\n",
    "\n",
    "**不同点：**\n",
    "1. nn.Module：会自动提取可学习的参数，位置-__init__() 构造函数中;\n",
    "2. nn.functional: 纯函数，不包含可学参数，位置-forward()前向传播中;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = V(t.randn(2, 3))\n",
    "model = nn.Linear(3, 4)\n",
    "output1 = model(input)\n",
    "output2 = nn.functional.linear(input, model.weight, model.bias)\n",
    "output1 == output2\n",
    "#nn.functional.linear??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nn.functional.relu(input)\n",
    "b2 = nn.ReLU()(input)\n",
    "b == b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时读者可能会问，应该什么时候使用nn.Module，什么时候使用nn.functional呢？答案很简单，**如果模型有可学习的参数，最好用nn.Module**，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。下面举例说明，如何在模型中搭配使用nn.Module和nn.functional。另外虽然dropout操作也没有可学习操作，但建议还是使用`nn.Dropout`而不是`nn.functional.dropout`，因为dropout在训练和测试两个阶段的行为有所差别，使用`nn.Module`对象能够通过`model.eval`操作加以区分。\n",
    "**使用场景：**\n",
    "1. nn.Module：模型有可学习参数，如卷积层、全连接层等具有可学习参数的网络等；（dropout操作无可学习参数，建议用nn.Dropout）\n",
    "2. nn.functional：个人喜好，性能没有太大差异，如激活函数（ReLU、sigmoid、tanh）、池化（MaxPool）等层由于没有可学习参数，则可以使用nn.functional函数代替；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pool(F.relu(self.conv1(x)), 2)\n",
    "        x = F.pool(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数`__init__`中。对于有可学习参数的模块，也可以用functional来代替，只不过实现起来较为**繁琐**，需要手动定义参数parameter，如前面实现自定义的全连接层，就可将weight和bias两个参数单独拿出来，在构造函数中初始化为parameter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(t.randn(3, 4))\n",
    "        self.bias = nn.Parameter(t.zeros(3))\n",
    "    def forward(self):\n",
    "        return F.linear(input, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于nn.functional的设计初衷，以及它和nn.Module更多的比较说明，可参看论坛的讨论和作者说明[^6]。\n",
    "[^6]: https://discuss.pytorch.org/search?q=nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 初始化策略\n",
    "在深度学习中参数的初始化十分重要，良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化则可能使得模型迅速瘫痪。PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略，因此一般不用我们考虑，当然我们也可以用自定义初始化去代替系统的默认初始化。而当我们在使用Parameter时，自定义初始化则尤为重要，因**t.Tensor()**返回的是**内存中的随机数**，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。PyTorch中`nn.init`模块就是专门为初始化而设计，如果某种初始化策略`nn.init`不提供，用户也可以自己直接初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3535,  0.1427,  0.0330],\n",
       "        [ 0.3321, -0.2416, -0.0888],\n",
       "        [-0.8140,  0.2040, -0.5493],\n",
       "        [-0.3010, -0.4769, -0.0311]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用nn.init初始化\n",
    "from torch.nn import init\n",
    "import sys\n",
    "linear = nn.Linear(3, 4)\n",
    "\n",
    "t.manual_seed(1)\n",
    "# 等价于 linear.weight.data.normal_(0, std)\n",
    "init.xavier_normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5345224838248488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3535,  0.1427,  0.0330],\n",
       "        [ 0.3321, -0.2416, -0.0888],\n",
       "        [-0.8140,  0.2040, -0.5493],\n",
       "        [-0.3010, -0.4769, -0.0311]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接初始化\n",
    "import math\n",
    "t.manual_seed(1)\n",
    "\n",
    "# xavier初始化的计算公式\n",
    "std = math.sqrt(2)/math.sqrt(7.)\n",
    "print(std)\n",
    "linear.weight.data.normal_(0,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对模型的所有参数进行初始化\n",
    "for name, params in net.named_parameters():\n",
    "    if name.find('linear') != -1:\n",
    "        # init linear\n",
    "        params[0] # weight\n",
    "        params[1] # bias\n",
    "    elif name.find('conv') != -1:\n",
    "        pass\n",
    "    elif name.find('norm') != -1:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 nn.Module深入分析\n",
    "\n",
    "如果想要更深入地理解nn.Module，究其原理是很有必要的。首先来看看nn.Module基类的构造函数：\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "def __init__(self):\n",
    "    self._parameters = OrderedDict()\n",
    "    self._modules = OrderedDict()\n",
    "    self._buffers = OrderedDict()\n",
    "    self._backward_hooks = OrderedDict()\n",
    "    self._forward_hooks = OrderedDict()\n",
    "    self.training = True\n",
    "```\n",
    "其中每个属性的解释如下：\n",
    "\n",
    "- `_parameters`：字典，保存用户直接设置的parameter，`self.param1 = nn.Parameter(t.randn(3, 3))`会被检测到，在字典中加入一个key为'param'，value为对应parameter的item。而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。\n",
    "- `_modules`：子module，通过`self.submodel = nn.Linear(3, 4)`指定的子module会保存于此。\n",
    "- `_buffers`：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。\n",
    "- `_backward_hooks`与`_forward_hooks`：钩子技术，用来提取中间变量，类似variable的hook。\n",
    "- `training`：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值来决定前向传播策略。\n",
    "\n",
    "上述几个属性中，`_parameters`、`_modules`和`_buffers`这三个字典中的键值，都可以通过`self.key`方式获得，效果等价于`self._parameters['key']`.\n",
    "\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (submodel1): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 等价与self.register_parameter('param1' ,nn.Parameter(t.randn(3, 3)))\n",
    "        self.param1 = nn.Parameter(t.rand(3, 3))\n",
    "        self.submodel1 = nn.Linear(3, 4) \n",
    "    def forward(self, input):\n",
    "        x = self.param1.mm(input)\n",
    "        x = self.submodel1(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('submodel1', Linear(in_features=3, out_features=4, bias=True))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('param1', Parameter containing:\n",
       "              tensor([[0.3398, 0.5239, 0.7981],\n",
       "                      [0.7718, 0.0112, 0.8100],\n",
       "                      [0.6397, 0.9743, 0.8300]], requires_grad=True))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3398, 0.5239, 0.7981],\n",
       "        [0.7718, 0.0112, 0.8100],\n",
       "        [0.6397, 0.9743, 0.8300]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.param1 # 等价于net._parameters['param1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1 torch.Size([3, 3])\n",
      "submodel1.weight torch.Size([4, 3])\n",
      "submodel1.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Net(\n",
      "  (submodel1): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "submodel1 Linear(in_features=3, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, submodel in net.named_modules():\n",
    "    print(name, submodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('running_mean', tensor([0.0514, 0.0749])),\n",
       "             ('running_var', tensor([0.9116, 0.9068])),\n",
       "             ('num_batches_tracked', tensor(1))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(2)\n",
    "input = V(t.rand(3, 2), requires_grad=True)\n",
    "output = bn(input)\n",
    "bn._buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为方便用户访问各个子module，nn.Module实现了很多方法，如函数`children`可以查看直接子module，函数`module`可以查看所有的子module（包括当前module）。与之相对应的还有函数`named_childen`和`named_modules`，其能够在返回module列表的同时返回它们的名字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = V(t.arange(0, 12).view(3, 4))\n",
    "input = t.arange(0, 12).view(3, 4)\n",
    "model = nn.Dropout()\n",
    "# 在训练阶段，会有一半左右的数被随机置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5)\n"
     ]
    }
   ],
   "source": [
    "model.training  = False\n",
    "# 在测试阶段，dropout什么都不做\n",
    "model(input)\n",
    "# 在训练阶段，会有一半左右的数被随机置为0\n",
    "print(model.train(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试时不将其training值设为True，则可能会有很大影响，这在实际使用中要千万注意。虽然可通过直接设置`training`属性，来将子module设为train和eval模式，但这种方式较为繁琐，因如果一个模型具有多个dropout层，就需要为每个dropout层指定training属性。更为推荐的做法是调用`model.train()`函数，它会将当前module及其子module中的所有training属性都设为True，相应的，`model.eval()`函数会把training属性都设为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.training, net.submodel1.training)\n",
    "net.eval()\n",
    "net.training, net.submodel1.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', Net(\n",
       "    (submodel1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  )), ('submodel1', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.named_modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`register_forward_hook`与`register_backward_hook`，这两个函数的功能类似于variable函数的`register_hook`，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：`hook(module, input, output) -> None`，而反向传播则具有如下形式：`hook(module, grad_input, grad_output) -> Tensor or None`。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。\n",
    "```python\n",
    "model = VGG()\n",
    "features = t.Tensor()\n",
    "def hook(module, input, output):\n",
    "    '''把这层的输出拷贝到features中'''\n",
    "    features.copy_(output.data)\n",
    "    \n",
    "handle = model.layer8.register_forward_hook(hook)\n",
    "_ = model(input)\n",
    "# 用完hook后删除\n",
    "handle.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module`对象在构造函数中的行为看起来有些怪异，如果想要真正掌握其原理，就需要看两个魔法方法`__getattr__`和`__setattr__`。在Python中有两个常用的buildin方法`getattr`和`setattr`，`getattr(obj, 'attr1')`等价于`obj.attr`，如果`getattr`函数无法找到所需属性，Python会转而调用`obj.__getattr__('attr1')`方法，即`getattr`函数无法找到的交给`__getattr__`函数处理，没有实现`__getattr__`或者`__getattr__`也无法处理的就会raise AttributeError。`setattr(obj, 'name', value)`等价于`obj.name=value`，如果obj对象实现了`__setattr__`方法，setattr会直接调用`obj.__setattr__('name', value)`，否则调用buildin方法。总结一下：\n",
    "- result  = obj.name会调用buildin函数`getattr(obj, 'name')`，如果该属性找不到，会调用`obj.__getattr__('name')`\n",
    "- obj.name = value会调用buildin函数`setattr(obj, 'name', value)`，如果obj对象实现了`__setattr__`方法，`setattr`会直接调用`obj.__setattr__('name', value')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module实现了自定义的`__setattr__`函数，当执行`module.name=value`时，会在`__setattr__`中判断value是否为`Parameter`或`nn.Module`对象，如果是则将这些对象加到`_parameters`和`_modules`两个字典中，而如果是其它类型的对象，如`Variable`、`list`、`dict`等，则调用默认的操作，将这个值保存在`__dict__`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param', Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('subModule', Linear(in_features=3, out_features=4, bias=True))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.Module()\n",
    "module.param = nn.Parameter(t.ones(2, 2))\n",
    "print(module._parameters)\n",
    "module.subModule = nn.Linear(3, 4)\n",
    "module._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_modules:  OrderedDict([('subModule', Linear(in_features=3, out_features=4, bias=True))])\n",
      "__dict__['submodules']: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "submodule1 = nn.Linear(2, 2)\n",
    "submodule2 = nn.Linear(2, 2)\n",
    "module_list =  [submodule1, submodule2]\n",
    "# 对于list对象，调用buildin函数，保存在__dict__中\n",
    "module.submodules = module_list\n",
    "print('_modules: ', module._modules)\n",
    "print(\"__dict__['submodules']:\",module.__dict__.get('submodules'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList is instance of nn.Module:  True\n",
      "_modules:  OrderedDict([('subModule', Linear(in_features=3, out_features=4, bias=True)), ('submodules', ModuleList(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "))])\n",
      "__dict__['submodules']: None\n"
     ]
    }
   ],
   "source": [
    "module_list = nn.ModuleList(module_list)\n",
    "module.submodules = module_list\n",
    "print('ModuleList is instance of nn.Module: ', isinstance(module_list, nn.Module))\n",
    "print('_modules: ', module._modules)\n",
    "print(\"__dict__['submodules']:\", module.__dict__.get('submodules'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因`_modules`和`_parameters`中的item未保存在`__dict__`中，所以默认的getattr方法无法获取它，因而`nn.Module`实现了自定义的`__getattr__`方法，如果默认的`getattr`无法处理，就调用自定义的`__getattr__`方法，尝试从`_modules`、`_parameters`和`_buffers`这三个字典中获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(module, 'training') # 等价于module.training\n",
    "# error\n",
    "# module.__getattr__('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.attr1 = 2\n",
    "getattr(module, 'attr1')\n",
    "# 报错\n",
    "# module.__getattr__('attr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即module.param, 会调用module.__getattr__('param')\n",
    "getattr(module, 'param')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型保存\n",
    "在PyTorch中保存模型十分简单，所有的Module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用`model.load_state_dict()`函数将状态加载进来。优化器（optimizer）也有类似的机制，不过一般并不需要保存优化器的运行状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "t.save(net.state_dict(), 'net.pth')\n",
    "\n",
    "# 加载已保存的模型\n",
    "net2 = Net()\n",
    "net2.load_state_dict(t.load('net.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上还有另外一种保存方法，但因其严重依赖模型定义方式及文件路径结构等，很容易出问题，因而不建议使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang/software/anaconda3/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (submodel1): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.save(net, 'net_all.pth')\n",
    "net2 = t.load('net_all.pth')\n",
    "net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Module放在GPU上运行也十分简单，只需两步：\n",
    "- model = model.cuda()：将模型的所有参数转存到GPU\n",
    "- input.cuda()：将输入数据也放置到GPU上\n",
    "\n",
    "至于如何在多个GPU上并行计算，PyTorch也提供了两个函数，可实现简单高效的并行GPU计算\n",
    "- nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)\n",
    "- class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "\n",
    "可见二者的参数十分相似，通过`device_ids`参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者则返回一个新的module，能够自动在多GPU上进行并行加速。\n",
    "\n",
    "```\n",
    "# method 1\n",
    "new_net = nn.DataParallel(net, device_ids=[0, 1])\n",
    "output = new_net(input)\n",
    "\n",
    "# method 2\n",
    "output = nn.parallel.data_parallel(new_net, input, device_ids=[0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，各个GPU得到的梯度累加。与Module相关的所有数据也都会以浅复制的方式复制多份，在此需要注意，在module中属性应该是只读的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 nn和autograd的关系\n",
    "nn.Module利用的也是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都是用到了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别：\n",
    "- autograd.Function利用了Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播\n",
    "- nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播\n",
    "- nn.functional是一些autograd操作的集合，是经过封装的函数\n",
    "\n",
    "作为两大类扩充PyTorch接口的方法，我们在实际使用中应该如何选择呢？如果某一个操作，在autograd中尚未支持，那么只能实现Function接口对应的前向传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，正如第三章所实现的`Sigmoid`一样，比直接利用autograd低级别的操作要快。而如果只是想在深度学习中增加某一层，使用nn.Module进行封装则更为简单高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 小试牛刀：搭建ResNet\n",
    "Kaiming He的深度残差网络（ResNet）[^7]在深度学习的发展中起到了很重要的作用，ResNet不仅一举拿下了当年CV下多个比赛项目的冠军，更重要的是这一结构解决了训练极深网络时的梯度消失问题。\n",
    "\n",
    "首先来看看ResNet的网络结构，这里选取的是ResNet的一个变种：ResNet34。ResNet的网络结构如图4-2所示，可见除了最开始的卷积池化和最后的池化全连接之外，网络中有很多结构相似的单元，这些重复单元的共同点就是有个跨层直连的shortcut。ResNet中将一个跨层直连的单元称为Residual block，其结构如图4-3所示，左边部分是普通的卷积网络结构，右边是直连，但如果输入和输出的通道数不一致，或其步长不为1，那么就需要有一个专门的单元将二者转成一致，使其可以相加。\n",
    "\n",
    "另外我们可以发现Residual block的大小也是有规律的，在最开始的pool之后有连续的几个一模一样的Residual block单元，这些单元的通道数一样，在这里我们将这几个拥有多个Residual block单元的结构称之为layer，注意和之前讲的layer区分开来，这里的layer是几个层的集合。\n",
    "\n",
    "考虑到Residual block和layer出现了多次，我们可以把它们实现为一个子Module或函数。这里我们将Residual block实现为一个子moduke，而将layer实现为一个函数。下面是实现代码，规律总结如下：\n",
    "\n",
    "- 对于模型中的重复部分，实现为子module或用函数生成相应的module`make_layer`\n",
    "- nn.Module和nn.Functional结合使用\n",
    "- 尽量使用`nn.Seqential`\n",
    "\n",
    "![图4-2: ResNet34网络结构](imgs/resnet1.png)\n",
    "![图4-3: Residual block 结构图](imgs/residual.png)\n",
    " [^7]: He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-778."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as t\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    实现子module: Residual Block\n",
    "    '''\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "                nn.Conv2d(inchannel,outchannel,3,stride, 1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(outchannel,outchannel,3,1,1,bias=False),\n",
    "                nn.BatchNorm2d(outchannel) )\n",
    "        self.right = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    实现主module：ResNet34\n",
    "    ResNet34 包含多个layer，每个layer又包含多个residual block\n",
    "    用子module来实现residual block，用_make_layer函数来实现layer\n",
    "    '''\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        # 前几层图像转换\n",
    "        self.pre = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(3, 2, 1))\n",
    "        \n",
    "        # 重复的layer，分别有3，4，6，3个residual block\n",
    "        self.layer1 = self._make_layer( 64, 64, 3)\n",
    "        self.layer2 = self._make_layer( 64, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer( 128, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer( 256, 512, 3, stride=2)\n",
    "\n",
    "        #分类用的全连接\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self,  inchannel, outchannel, block_num, stride=1):\n",
    "        '''\n",
    "        构建layer,包含多个residual block\n",
    "        '''\n",
    "        shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel,outchannel,1,stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))\n",
    "        \n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(outchannel, outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3699, -0.0577, -0.2139, -0.2069,  0.6409, -0.3869, -0.8199,  0.1431,\n",
       "          0.3655, -0.1840,  0.1636,  0.0792,  0.2667, -0.2909, -0.4293, -0.2775,\n",
       "         -0.0003, -0.1377,  0.5383, -0.1442,  0.2079,  0.1935, -0.5733, -0.4060,\n",
       "          0.0183,  0.1847,  0.0045, -0.7078,  0.0822,  0.0045, -0.1254, -0.0863,\n",
       "          0.2353,  0.9867,  0.2410, -0.3185, -0.0818,  0.0273, -0.5187,  0.1836,\n",
       "          0.5195,  0.2716,  0.1647,  0.0679,  0.6863,  0.4060, -0.0253,  0.1288,\n",
       "         -0.3285, -0.2558,  0.5720,  0.0742,  0.3389, -0.4069, -0.1116,  0.1423,\n",
       "          0.0040, -0.1440, -0.0743,  0.1665,  0.1521, -0.2475,  0.1781,  0.1629,\n",
       "         -0.1038, -0.2036, -0.1833,  0.4475,  0.0306, -0.2050,  0.7638, -0.5444,\n",
       "         -0.0892, -0.2503,  0.5010, -0.2374,  0.7362, -0.0051,  0.3687,  0.2311,\n",
       "          0.3877, -0.4474, -0.5796,  0.4938, -0.0532, -0.3328,  0.1256, -0.1588,\n",
       "         -0.5438,  0.2185,  0.2074, -0.2901,  0.1835,  0.1082, -0.0221, -0.1460,\n",
       "          0.3471, -0.0320,  0.0322,  0.0229, -0.1827,  0.7973,  0.1503,  0.4819,\n",
       "         -0.1052, -0.0800, -0.1568, -0.1021,  0.1430, -0.3287,  0.0656,  0.5681,\n",
       "          0.2550, -0.2646,  0.0103,  0.0253, -0.6937, -0.2063,  0.1372,  0.1732,\n",
       "         -0.0116, -0.2575, -0.0483,  0.4716, -0.0484, -0.3572,  0.0812, -0.2511,\n",
       "         -0.2195,  0.1115,  0.0582, -0.2633, -0.6301, -0.1536, -0.7812, -0.0020,\n",
       "         -0.0875,  0.2824,  0.7023, -0.0407,  0.1440, -0.2076, -0.0302, -0.0639,\n",
       "          0.3864,  0.3352, -0.2658, -0.2623, -0.3977,  0.2534, -0.6040, -0.3537,\n",
       "          0.3543, -0.6228, -0.1858,  0.5272,  0.3531,  0.0473,  0.0835, -0.3493,\n",
       "          0.1248, -0.4276, -0.0524,  0.3411, -0.0870,  0.2315, -0.5265, -0.3038,\n",
       "         -0.1240,  0.1632,  0.4881,  0.1677, -0.1745,  0.3655, -0.3209, -0.3111,\n",
       "         -0.0771,  0.0751, -0.2150,  0.0665,  0.1370,  0.2278, -0.1140,  0.9141,\n",
       "          0.0951, -0.3026, -0.0259, -0.0495, -0.2148, -0.4735,  0.3421, -0.3824,\n",
       "         -0.6506,  0.4159, -0.1790, -0.3118,  0.0470,  0.2904,  0.2015, -0.1573,\n",
       "         -0.2377, -0.0614, -0.6592,  0.0943, -0.3682, -0.2187,  0.7422,  0.2422,\n",
       "          0.1317,  0.2585,  0.0347,  0.4432,  0.2141, -0.4597,  0.5348,  0.1868,\n",
       "         -0.1613,  0.6068,  0.3888, -0.0721, -0.2832, -0.1675, -0.0167,  0.6142,\n",
       "          0.2563, -0.2290,  0.0768,  0.0967,  0.1723, -0.0632, -0.5120, -0.4556,\n",
       "          0.5133,  0.2306,  0.0503,  0.3645,  0.1468, -0.1272, -0.1013,  0.2745,\n",
       "          0.1558, -0.3080,  0.2922, -0.5141, -0.0321, -0.7985, -0.2145, -0.6368,\n",
       "          0.2757, -0.0120,  0.3584,  0.7485,  0.1261,  0.0767, -0.0040,  0.0802,\n",
       "         -0.0090,  0.6719,  0.4693, -0.0081, -0.1557, -0.4340, -0.1579,  0.2257,\n",
       "          0.1055, -0.5235, -0.3144,  0.3786, -0.4973,  0.2840,  0.2724, -0.3044,\n",
       "          0.0307,  0.3129,  0.0600,  0.0665,  0.6158, -0.1795,  0.1808,  0.2741,\n",
       "         -0.1474,  0.2267,  0.0140,  0.0474,  0.4388,  0.3880,  0.2479,  0.2939,\n",
       "         -0.1384, -0.4727, -0.1277, -0.8570, -0.4172,  0.2850, -0.1389,  0.1837,\n",
       "          0.0144, -0.6055, -0.3498,  0.3362,  0.2811, -0.1539, -0.4370, -0.0344,\n",
       "         -0.2891,  0.1876, -0.7768, -0.2360, -0.2341, -0.3945, -0.0385,  0.0160,\n",
       "          0.5002, -0.3050,  0.1683,  0.6457,  0.2443, -0.0592, -0.4993, -0.2276,\n",
       "         -0.0661, -0.3623, -0.5668,  0.1213,  0.5283, -0.3325, -0.2658,  0.1557,\n",
       "         -0.4356,  0.2055, -0.2149, -0.2050, -0.4995, -0.3510, -0.0598, -0.5904,\n",
       "         -0.0190, -0.5096, -0.0401,  0.1297, -0.0741,  0.2473, -0.5404,  0.4737,\n",
       "          0.2684,  0.0315,  0.1719,  0.2114,  0.2275, -0.6102,  0.1756, -0.0782,\n",
       "         -0.2852,  0.5025,  0.7275, -0.2935, -0.1882, -0.0943, -0.4714, -0.2422,\n",
       "         -0.2421, -0.1313,  0.0262,  0.6713, -0.1250,  0.4389, -0.7253,  0.0088,\n",
       "         -0.6918,  0.0889, -0.5677,  0.0075,  0.2980, -0.7701,  0.1455,  0.5967,\n",
       "          0.3414,  0.1303, -0.5470, -0.3160, -0.7955,  0.6063,  0.6130,  0.4181,\n",
       "         -0.2155,  0.1307,  0.0424,  0.0467,  0.0470, -0.4931,  0.0292,  0.2070,\n",
       "          0.0374,  0.7232, -0.9953,  0.3442,  0.0644,  0.4681,  0.4041, -0.0495,\n",
       "          0.1955, -0.2172,  0.3335,  0.0979, -0.2975,  0.3039,  0.3676,  0.1980,\n",
       "         -0.3422,  0.0104,  0.3014,  0.0158, -0.4303, -0.2366,  0.0630, -0.1820,\n",
       "          0.3200,  0.3575,  0.1130,  0.0472, -0.6759, -0.0607, -0.0244,  0.0380,\n",
       "          0.3392,  0.4232, -0.1406,  0.0088,  0.3229, -0.3803,  0.0821,  0.0746,\n",
       "          0.3617, -0.4351, -0.1213,  0.0920, -0.4615,  0.0261,  0.2196,  0.2735,\n",
       "         -0.2386,  0.1002, -0.0550, -0.0663, -0.2673, -0.2072,  0.5371, -0.1437,\n",
       "          0.3517,  0.5851,  0.0918,  0.0389,  0.0865, -0.2328, -0.0927, -0.2854,\n",
       "          0.7633,  0.1471, -0.0960, -0.5398, -0.2396, -0.0261,  0.0092, -0.3307,\n",
       "         -0.3282,  0.0471,  0.3919,  0.3647,  0.1986, -0.3519,  0.9480, -0.5069,\n",
       "         -0.0281, -0.0975, -0.6315, -0.1539, -0.2999,  0.1056, -0.7793,  0.5411,\n",
       "          0.2384, -0.1944, -0.3838, -0.0732,  0.3175,  0.3262, -0.2827, -0.5350,\n",
       "         -0.2812,  0.4416, -0.4797,  0.0417, -0.2673,  0.0794, -0.1564, -0.5435,\n",
       "         -0.0576,  0.0329, -0.4096, -0.0114,  0.3064,  0.3130, -0.2381,  0.6935,\n",
       "         -0.3840, -0.7074, -0.2044,  0.2024, -0.2538, -0.0431, -0.5054, -0.0617,\n",
       "          0.4226,  0.7542, -0.0204, -0.0987, -0.0931, -0.1440, -0.7415, -0.0135,\n",
       "         -0.0444,  0.4512,  0.1717,  0.0792,  0.0325,  0.3949,  0.1550, -0.0597,\n",
       "         -0.6591, -0.0422, -0.2998,  0.4186, -0.0830,  0.0842, -0.1914, -0.2061,\n",
       "          0.1603,  0.2275,  0.0276, -0.1079, -0.1842,  0.3040, -0.0490, -0.0772,\n",
       "          0.5178,  0.0469, -0.0875,  0.0582, -0.4291, -0.1224,  0.4586, -0.1957,\n",
       "         -0.0449,  0.9319,  0.6304, -0.0634,  0.0365,  0.1433,  0.0141,  0.5434,\n",
       "         -0.2285, -0.1075,  0.2562,  0.6627, -0.3301,  0.0358,  0.0230,  0.3828,\n",
       "         -0.8586,  0.3279, -0.7432, -0.2359, -0.2127, -0.0075,  0.5483, -0.3653,\n",
       "          0.4260,  0.1667,  0.6214,  0.0658, -0.2729, -0.0282,  0.0069, -0.0375,\n",
       "         -0.0565,  0.1062, -0.4531, -0.5149, -0.1944, -0.4615, -0.1947,  0.3924,\n",
       "          0.4101,  0.3730, -0.0263, -0.0700,  0.2153,  0.3806, -0.4811, -0.5342,\n",
       "          0.4732, -0.2430,  0.6658, -0.3087,  1.0305,  0.2937, -0.2133,  0.0186,\n",
       "          0.3595,  0.0661, -0.2883, -0.6077, -0.3365, -0.5970, -0.3233,  0.3941,\n",
       "          0.0244, -0.1974, -0.0690,  0.3054,  0.0220, -0.2073,  0.5293, -0.3356,\n",
       "         -0.3941,  0.2544,  0.8435,  0.2032, -0.4172,  0.2596, -0.4889, -0.0970,\n",
       "          0.0149, -0.1605,  0.0965,  0.0938,  0.4325,  0.1894, -0.9368, -0.3722,\n",
       "          0.1259,  0.2180, -0.1576, -0.0455,  0.0679, -0.2570, -0.1703,  0.2159,\n",
       "          0.3379,  0.4603, -0.2804,  0.9621,  0.2710,  0.3010,  0.4322, -0.2042,\n",
       "         -0.3096,  0.1666, -0.0595, -0.1676,  0.1099, -0.2381, -0.3494, -0.7203,\n",
       "         -0.3342,  0.7303,  0.4463, -0.2183,  0.4302, -0.1772,  0.0696, -0.1442,\n",
       "          0.2886,  0.0028, -0.3509, -0.1560,  0.2606, -0.1071, -0.0170, -0.3682,\n",
       "         -0.0471,  0.0833,  0.4902,  0.1489, -0.1899,  0.4102,  0.0155,  0.6440,\n",
       "         -0.4432,  0.2474,  0.4109,  0.1521, -0.0309,  0.0369,  0.7955, -0.4432,\n",
       "          0.0854, -0.2786, -0.0043, -0.0312,  0.2983,  0.1916,  0.1718,  0.1155,\n",
       "         -0.0531,  0.1694, -0.1871, -0.5381, -0.4246,  0.2799,  0.3951, -0.0722,\n",
       "          0.0968, -0.2236, -0.2324, -0.4244,  0.0536, -0.1874,  0.1554, -0.1011,\n",
       "          0.6902, -0.2946,  0.2048, -0.0676,  0.0100, -0.5379,  0.2083, -0.0482,\n",
       "          0.2335,  0.6139,  0.1674, -0.2648, -0.2953, -0.3279, -0.3884,  0.1435,\n",
       "          0.1051, -0.0782,  0.0270,  0.9623, -0.2694,  0.1536, -0.1157,  0.1254,\n",
       "          0.7506,  0.1115, -0.4389,  0.2296,  0.1944, -0.0834, -0.1092,  0.0996,\n",
       "         -0.1713, -0.3781,  0.7081, -0.1581,  0.2624, -0.2746,  0.2553,  0.1256,\n",
       "         -0.0226, -0.1569,  0.0541,  0.1131, -0.0352,  0.3540,  0.3589,  0.2851,\n",
       "         -0.1259, -0.0858,  0.0930, -0.0258,  0.1149,  0.6546,  0.3396, -0.2023,\n",
       "          0.5922, -0.2583,  0.0190, -0.1515, -0.2207,  0.2216, -0.8303, -0.3585,\n",
       "          0.1008, -0.3054, -0.3076,  0.1906,  0.4109, -0.2107,  0.6324,  0.0697,\n",
       "         -0.0352, -0.1512,  0.1846,  0.3462, -0.1669,  0.0574,  0.4903,  0.3163,\n",
       "         -0.0088,  0.1418, -0.2216,  0.2021,  0.2860,  0.4253,  0.5551, -0.0990,\n",
       "         -0.0420, -0.0512,  0.4627, -0.1026,  0.0961, -0.8385,  0.1510, -0.6020,\n",
       "         -0.3140, -0.4060, -0.1091, -0.2980, -0.2552, -0.1125, -0.0832,  0.5609,\n",
       "         -0.0032,  0.1803, -0.0056, -0.1493,  0.0592,  0.1726, -0.2511, -0.1188,\n",
       "         -0.2001,  0.6683,  0.2723,  0.1587, -0.1783,  0.7626, -0.1513, -0.5282,\n",
       "          0.4580,  0.5249,  0.6226,  0.4688,  0.0066,  0.1633,  0.4295,  0.0420,\n",
       "          0.0207,  0.0037, -0.0507, -0.2163, -0.7068,  0.5620, -0.1067, -0.3376,\n",
       "         -0.1480, -0.0836, -0.3252, -0.2033, -0.1925, -0.2455, -0.0351,  0.0382,\n",
       "         -0.1145, -0.3281,  0.9791,  0.1883,  0.1957,  0.8657, -0.1486,  0.0719,\n",
       "          0.1751,  0.2522,  0.2004,  0.6005, -0.3190,  0.3685,  0.2433, -0.8307,\n",
       "          0.2462, -0.1827,  0.2467,  0.1920,  0.0134,  0.5377, -0.7048,  0.4014,\n",
       "         -0.2735, -0.1087,  0.0255, -0.5177,  0.1475,  0.7356,  0.2188,  0.0696,\n",
       "          0.0013, -0.5167, -1.0267,  0.4225, -0.1863, -0.3038,  0.6381,  0.0948,\n",
       "         -0.0885, -0.3726,  0.7255, -0.0775, -0.4757,  0.5341,  0.0345, -0.1838,\n",
       "         -0.1425,  0.5100,  0.3128, -0.2315,  0.0840, -0.1022,  0.1133,  0.4134,\n",
       "          0.1320,  0.1032, -0.3559, -0.1385, -0.4590, -0.2168, -0.4429, -0.8376,\n",
       "          0.8273,  0.6826, -0.0396,  0.0755, -0.3490, -0.3304, -0.0878,  0.1131,\n",
       "          0.0523,  0.4166, -0.5916,  0.4902, -0.0580,  0.5516, -0.4306,  0.2686,\n",
       "          0.5830,  0.3391,  0.1872, -0.2935, -0.0974, -0.1875,  0.4768,  0.3016,\n",
       "         -0.1932,  0.3069, -0.4800, -0.5098, -0.4429, -0.0294,  0.6410,  0.5343,\n",
       "          0.2198,  0.0150, -0.0882, -0.2929, -0.2004,  0.5088,  0.5485,  0.0792,\n",
       "          0.0739, -0.2069, -0.2149,  0.2948, -0.1271,  0.7469, -0.1926,  0.1005,\n",
       "         -0.2094, -0.5032, -0.0403,  0.1079, -0.0268,  0.5809, -0.1195, -0.3681,\n",
       "         -0.1279,  0.2429,  0.9260, -0.1142, -0.1285,  0.0965, -0.2463,  0.2432,\n",
       "         -0.3622, -0.5356, -0.2437,  0.5186,  0.1526,  0.3801,  0.5929, -0.0508]],\n",
       "       grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet()\n",
    "input  = t.autograd.Variable(t.randn(1, 3, 224, 224))\n",
    "o = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感兴趣的读者可以尝试实现Google的Inception网络结构或ResNet的其它变体，看看如何能够简洁明了地实现它，实现代码尽量控制在80行以内（本例去掉空行和注释总共不超过50行）。另外，与PyTorch配套的图像工具包`torchvision`已经实现了深度学习中大多数经典的模型，其中就包括ResNet34，读者可以通过下面两行代码使用：\n",
    "```python\n",
    "from torchvision import models\n",
    "model = models.resnet34()\n",
    "```\n",
    "本例中ResNet34的实现就是参考了torchvision中的实现并做了简化，感兴趣的读者可以阅读相应的源码，比较这里的实现和torchvision中实现的不同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
