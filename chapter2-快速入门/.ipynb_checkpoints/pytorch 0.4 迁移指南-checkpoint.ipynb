{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考资料](https://www.cnblogs.com/z1141000271/p/9473096.html)\n",
    "\n",
    "[Pytorch 0.4.0迁移指南](https://www.pytorchtutorial.com/pytorch-0-4-0-migration-guide/)\n",
    "\n",
    "## 涉及主要变化点：\n",
    "- Tensor 和 Variable 合并了\n",
    "- Tensor 支持 0 维度（即标量）\n",
    "- 放弃使用 volatile\n",
    "- dtypes，devices 和 Numpy-style 的 Tensor 初始函数\n",
    "- 设备无关性\n",
    "- 一些新的参数命名规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor和Variable的合并\n",
    "&emsp;&emsp;旧版本（0.1~0.3）观点认为：当前Tensor为 requires_grad = False的 Variable的特例，torch.Tensor 和 torch.autograd.Variable是同一个类，没有本质的区别.\n",
    "实际上，已经没有纯粹的Tensor，所有的Tensor对象都应支持自动求导。\n",
    "`torch.Tensor` 和 `orch.autograd.Variable` 现在是一样的了。torch.Tensor 可以像以前的 Variable 一样记录历史数据，Variable 还可以继续使用，但会返回一个 torch.Tensor 类型的变量\n",
    "> 查看pytorch的版本类型\n",
    "```\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensor的类型\n",
    "&emsp;&emsp; 使用「`isinstance()`」或者「`x.type()`」,但是使用python内置函数type(x)不能查看tensor变量的实际类型；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.DoubleTensor\n",
      "<class 'torch.Tensor'>\n",
      "the variable x is a torch.FloatTensor:  False\n"
     ]
    }
   ],
   "source": [
    "x = t.DoubleTensor([1, 1, 1])\n",
    "print(x.type()) # was torch.DoubleTensor\n",
    "\n",
    "print(type(x))\n",
    "print('the variable x is a torch.FloatTensor: ', isinstance(x, t.FloatTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的requires_grad属性\n",
    "&emsp;&emsp;已经没有纯粹的tensor变量，任何一个tensor对象/变量都支持自动求导,默认求导属性为False，即 requires_grad = False;\n",
    "### requires_grad属性使用\n",
    "- 可将 requires_grad属性作为参数，构造tensor变量\n",
    "- 原地设定：my_tensor.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(1)\n",
    "x.requires_grad  # 默认为False\n",
    "y = t.ones(1)\n",
    "z = x + y\n",
    "z.requires_grad  # 根据传导性，z的 requires_grad属性 为False\n",
    "# z.backward()     # 因为所有变量都不需要grad，因此会 Error\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad_())\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum variable requires_grad and grad_fn is : \n",
      " True <ThAddBackward object at 0x7f1bc7a9a5c0>\n"
     ]
    }
   ],
   "source": [
    "# 将 requires_grad属性作为参数，构造 tensor变量\n",
    "w = t.ones(1, requires_grad = True)\n",
    "total = x + w\n",
    "print('the sum variable requires_grad and grad_fn is : \\n', total.requires_grad, total.grad_fn)\n",
    "\n",
    "# 此时可以 进行 求导，调用 backward函数\n",
    "total.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative is :  tensor([1.]) None\n",
      "tensor([1.]) None None None\n"
     ]
    }
   ],
   "source": [
    "print('the derivative is : ',w.grad,x.grad)\n",
    "\n",
    "# 因为 x,y,z 都是不需要梯度的（ requires_grad = False）,因此 x.grad,y.grad,z.grad均为None未进行计算\n",
    "print(w.grad,x.grad,y.grad,z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **`.data`**不要随意使用\n",
    "&emsp;&emsp;早期版本中，使用.data获得Variable变量中的Tensor。后期，Tensor和Variable进行了合并，因此「.data」返回一个新的requires_grad = False的Tensor！！！\n",
    "\n",
    "&emsp;&emsp;`.data`是从`Variable`中获取`Tensor`的主要方法，合并后，调用`y = x.data`后`y` 和 `x`共享相同的数据，但是 `y`和`x`的计算历史无关，而且`requires_grad=False`；\n",
    "\n",
    "**注意：**新的Tensor同之前的Tensor变量共享内存，所以不安全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time x.requires_grad =  True\n",
      "after time x.requires_grad =  True\n",
      "after time x.requires_grad =  True\n"
     ]
    }
   ],
   "source": [
    "print('before time x.requires_grad = ', x.requires_grad)\n",
    "y = x.data # x 需要进行 autograd\n",
    "print('after time x.requires_grad = ', x.requires_grad)\n",
    "# y和x 共享内容，但是这里并不需要grad了\n",
    "# 所以会导致本来需要进行梯度的x也没有梯度可以计算了，从而x不会得到更新\n",
    "z = x.detach()\n",
    "print('after time x.requires_grad = ', x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "&emsp;&emsp;`.data`在某些情况下不安全。`x.data`上的任何变化都不被`autograd`跟踪，并且在向后传播的过程中 `x`的值将出现差错。一种更安全的替代方法是使用`x.detach()`,它同样也会返回一个`requires_grad = False`的`Tensor`，**但** 它会有 **梯度变化的信息**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "out is :  tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n"
     ]
    }
   ],
   "source": [
    "# 使用 Tensor.detach(),梯度计算一定是正确的\n",
    "import torch\n",
    "print()\n",
    "a = torch.tensor([1,2,3.], requires_grad = True) # 默认数据类型为 DoubleTensor\n",
    "out = a.sigmoid()\n",
    "print('out is : ' , out)\n",
    "c = out.detach()\n",
    "print(c)\n",
    "c.zero_()\n",
    "out # modified by c.zero_() \n",
    "# out 计算梯度值，但是因为c.zeros_() 已经覆盖了 内容，报错\n",
    "total = out.sum()\n",
    "# total.backward() # Requires the original value of out, but that was overwritten by c.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是用 `Tensor.data`是不安全的，当引用的tensor被修改后，原始tensor也会被修改，会导致梯度计算出错;(而非报错）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "out = a.sigmoid()\n",
    "c = out.data\n",
    "c.zero_() # out will be overwrite by  c.zeros_()\n",
    "out # wad tensor([0., 0., 0.], grad_fn=<SigmoidBackward>)\n",
    "out.sum().backward() # 计算梯度;\n",
    "a.grad # the result is very, very wrong because `out` changed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor支持 0维度： scale 的支持\n",
    "&emsp;&emsp;早期版本中，一维`Tensor`的索引会返回一个数值，即返回一个number类型（标量）；但是indexing 一个Variable 确实返回一个size = （1，） 的 **vector**；再比如，tensor.sum()返回一个number,但variable.sum()返回的是一个size = （1，）的vector。\n",
    "&emsp;&emsp;引入了适当的标量（0维张量）支持，不像原来单个数据还给搞出一个一维数组（注意：(1,)此为一维数组）!!!现在可以使用新 `torch.tensor`函数来创建标量（scalar）。\n",
    "\n",
    "**好处：** \n",
    "1. 通过引入`scalar`，可以将返回值的类型进行统一；\n",
    "2. 取得一个tensor的值（返回number），需要使用成员函数「.item()」\n",
    "3. 创建 scalar（标量） 的话， 需要使用 「torch.tensor(number)」\n",
    "4. torch.tensor（list）也可以创建 tensor\n",
    "\n",
    "**注意：** \n",
    "- tensor.sum(): tensor表示一个Tensor类型的变量\n",
    "- variable.sum(): variable表示一个Variable类型的变量\n",
    "- scalar 打印出来是没有 **[ ]**的，其size为[]， tensor 打印出来会用 `[]`包上\n",
    "- scalar: 0-dimension 的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar size is :  torch.Size([])\n",
      "linear array size is ： torch.Size([1])\n",
      "the vector is :  tensor([2, 3, 4, 5])\n",
      "vector[3] =  tensor(5)\n",
      "vector[3].item() =  5\n",
      "mySum =  tensor(12)\n",
      "size(mySum) =  torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "torch.tensor(3.141592627) # 使用 torch.tensor来创建scalar\n",
    "print('scalar size is : ', torch.tensor(3.1415).size())  # size为 0 , was torch.Size([])\n",
    "print('linear array size is ：',torch.tensor([3]).size())    # compare to a vecotor of size 1\n",
    "#torch.tensor([2])          # 如果为 tensor,打印出来用 `[]`包上\n",
    "vector = torch.arange(2, 6) # this is a vector\n",
    "print('the vector is : ', vector) # 类似为： vector = torch.tensor([2, 3, 4, 5])\n",
    "\n",
    "# 获取 tensor中的值（vector） \n",
    "vector[3].item()   # 需要使用成员函数 「.item()」获取里面的值\n",
    "print('vector[3] = ',vector[3])\n",
    "print('vector[3].item() = ', vector[3].item())\n",
    "\n",
    "# 而这种 reduction 操作，返回的是一个 scalar (0-dimension 的 tensor)\n",
    "mySum = torch.tensor([2, 4, 6]).sum()\n",
    "print('mySum = ', mySum)\n",
    "print('size(mySum) = ', mySum.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "## 累加 loss\n",
    "&emsp;&emsp;早期版本中，累加loss一般使用 「totalLoss += loss.data[0]」累积损失率，在0.4版本中有0维的标量，直接用`loss.item()`得到其 loss 的数值就可以了\n",
    "#### 为啥采用 「.data[0]」\n",
    "**原因：**loss是一个Variable，如果在使用 累加损失时未将 其转化为 Python数字（即 标量），则可能出现程序内存使用量增加的情况。\n",
    "\n",
    "&emsp;&emsp;这是因为上面表达式的右侧原本是一个 Python浮点数，而现在它是一个 `零维张量`。因此，总损失累加了张量和 **它们的梯度历史**，这可能会使autograd图 保存比我们所需要长的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 弃用 volatile（用于告诉 `autograd`跟踪那些Variable—根据其 volatile= True）\n",
    "\n",
    "&emsp;&emsp;之前，`autograd` 不会跟踪任何涉及 `Variable(volatile=True)`的计算。当前版本，已经被替换为一套更加灵活的上下文管理器，如： `torch.no_grad()`，`torch.set_grad_enabled(grad_mode)`等等;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size is :  torch.Size([1])\n",
      "y.requires_grad is :  True\n",
      "has enter the with function\n",
      "z.requires_grad is :  False\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad = True)\n",
    "print('x.size is : ', x.size()) # was tensor, not scalar；\n",
    "y = x ** 2\n",
    "print('y.requires_grad is : ', y.requires_grad)\n",
    "with torch.no_grad():\n",
    "    z = x ** 2\n",
    "    print('has enter the with function')\n",
    "print('z.requires_grad is : ', z.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.requires_grad is :  True\n",
      "has enter the with function\n",
      "n.requires_grad is :  False\n",
      "w.requires_grad is :  True\n"
     ]
    }
   ],
   "source": [
    "is_train = False\n",
    "m = x ** 2\n",
    "print('m.requires_grad is : ', m.requires_grad)\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    n = x ** 2\n",
    "    print('has enter the with function')\n",
    "print('n.requires_grad is : ', n.requires_grad)\n",
    "# 仅仅针对 with 中内容，如果设置全局的，可以直接调用函数\n",
    "# torch.set_grad_enable(True)\n",
    "w = x**2\n",
    "print('w.requires_grad is : ', w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True) # this can also used as a funciton\n",
    "y = x ** 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dtypes, devices 以及 numpy-style 的构造函数\n",
    "&emsp;&emsp;之前pytorch版本中，我们用`数据类型`(float vs double)、`设备类型`（cpu vs cuda）和 `layout`（dense 和 sparse）共同作为 **张量类型**。\n",
    "> 例如：`torch.cuda.sparse.DoubleTensor`是`Tensor`的 `double`数据类型，支持`CUDA`，以及支持 COO Sparse Tensor Layout。\n",
    "\n",
    "**在0.4.0版本中**，引入 `torch.dtype`、`torch.device`以及`torch.layout`类，通过`Numpy` 的风格更好的管理这些类型。\n",
    "\n",
    "## torch.dtype\n",
    "\n",
    "下图是 dtype的完整列表以及对应的Tensor类型:\n",
    "\n",
    "![image](https://github.com/stonels0/pytorch-book/tree/master/chapter2-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/imgs/dtype.jpeg) \n",
    "\n",
    "- 一个tensor变量的 `dtype`可以通过`tensor.dtype`获得（Numpy风格）\n",
    "\n",
    "## torch.device\n",
    "&emsp;&emsp;`torch.device`包含设备类型（`cpu`或`cuda`）和可选设备序号（`id`）。\n",
    "- 初始化：（{*}为替换内容）\n",
    " 1.  `torch.device('{device_type}')`\n",
    " 2.  `torch.device( '{device_type}:{device_ordinal}' )`\n",
    " \n",
    "> 如果没有设备序号，则表示为当前设备。例如，`torch.device('cuda')`等同于`torch.device('cuda:X')`，这里的X是`torch.cuda.current_device()`。\n",
    "> 张量设备可以通过tensor.device 获得\n",
    "\n",
    "## torch.layout\n",
    "&emsp;&emsp;torch.layout 代表一个 `Tensor`的数据布局。\n",
    "\n",
    "&emsp;&emsp; 张量的布局则可以通过 `tensor.layout`获得。\n",
    "\n",
    "\n",
    "通过**.dtype**可以得到，其他就是以前写 device.type 都是用 .cup() 或者 .cudn() ，现在独立成为一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "x = torch.randn(3, 3, dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建新的 Tensor方法\n",
    "&emsp;&emsp; 现在创建Tensor的方法，还包括**`dtype`、`device`、`layout`**和 **`requires_grad`**选项来指定返回所需Tensor所需的属性:\n",
    "1. 可以指定dtype 和 device 创建;\n",
    "2. 用 torch.tensor 创建 Tensor；\n",
    "3. 用 torch. * like 以及 torch.new_* 进行创建;\n",
    "\n",
    "如果需要创建指定尺寸的Tensor，可以直接用 **元组**指定尺寸作为参数，例如`torch.zeros(*(2,3)*)`或 `torch.zeros(2,3)`，这样就可以创建尺寸为 `2*3`,元素为 0 的Tensor。\n",
    "\n",
    "&emsp;&emsp; like: 用于创建 「shape相同，数据类型相同」\n",
    "\n",
    "&emsp;&emsp; new_：用于创建属性相同，shape不想要一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tensor variable device is :  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 dtype 和 device进行创建\n",
    "device = torch.device(\"cuda:0\")\n",
    "x = torch.randn(3, 3, dtype = torch.float64,device=device) # 当前使用device参数报错，后期研究 TODO\n",
    "# x.requires_grad # was False\n",
    "print('the tensor variable device is : ', x.device)\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 创建Tensor方法—torch.tensor()\n",
    "**torch.tensor是新增张量创建方法之一：极力推荐使用这种方法将已有的数据类（如list）转化为Tensor**\n",
    "&emsp;&emsp;`torch.tensor()`就像`numpy.array()`构造器，可以将数组类数据直接转化为Tensor,0.4.0这个版本中这个函数也可以构造标量。\n",
    "&emsp;&emsp;若初始化没有指定`dtype`数据类型，Pytorch将自动分配合适类型;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the x is :  tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]], device='cuda:0', dtype=torch.float16)\n",
      "torch.tensor([1, 2.3]).dtype =  torch.float32\n",
      "torch.tensor([1, 2]).dtype =  torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.tensor创建Tesor，类似于 numpy.array()\n",
    "# 从列表中list的数据来创建\n",
    "cuda = torch.device(\"cuda\")\n",
    "x = torch.tensor([[1], [2], [3]], dtype = torch.half, device=cuda)\n",
    "print('the x is : ', x)\n",
    "# 创建 scalar \n",
    "torch.tensor(1)  # scalar 打印输出 不包含【】\n",
    "print('torch.tensor([1, 2.3]).dtype = ', torch.tensor([1, 2.3]).dtype)\n",
    "print('torch.tensor([1, 2]).dtype = ', torch.tensor([1, 2]).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 创建Tensor方法—torch.*_like()\n",
    "\n",
    "&emsp;&emsp;接受Tensor数据(**注意不是数据的尺寸**)，如果不设置相关参数它默认返回一个具有相同属性的Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is :  tensor([-1.8337,  0.1578,  1.6724], dtype=torch.float64)\n",
      "torch.zeros_like(x) :  tensor([0., 0., 0.], dtype=torch.float64)\n",
      "torch.zeros_like(x, dtype = torch.int) :  tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# torch.*like : 创建「shape、数据类型」相同\n",
    "x = torch.randn(3, dtype = torch.float64)\n",
    "print('x is : ', x)\n",
    "print('torch.zeros_like(x) : ', torch.zeros_like(x))\n",
    "print('torch.zeros_like(x, dtype = torch.int) : ',torch.zeros_like(x, dtype = torch.int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 创建Tensor方法—torch.new_*()\n",
    "\n",
    "&emsp;&emsp;使用**尺寸**作为参数创建具有相同属性的Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is :  tensor([-0.1975,  0.7622, -0.8122], dtype=torch.float64)\n",
      "x.new_ones(2) is :  tensor([1., 1.], dtype=torch.float64)\n",
      "x.new_ones(4,dtype=torch.int)\n",
      "tensor([1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, dtype=torch.float64)\n",
    "print('x is : ',x)\n",
    "print('x.new_ones(2) is : ',x.new_ones(2))\n",
    "print('x.new_ones(4,dtype=torch.int)')\n",
    "y = x.new_ones(2) # 属性相同,均为 torch.float64类型的\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设备无关性\n",
    "> 当不确定计算设备的情况时，0.4版本做出如下更新：\n",
    "  1. 使用 `to`方法可以轻松转化训练的网络（module）和数据到不同计算设备运行\n",
    "  2. `device`属性用来指定使用的计算设备，之前要用 `cpu()`，`cuda()`转化模型或者数据\n",
    "  \n",
    "**在运行脚本开头写入如下代码**\n",
    "```\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用时，data和model添加to方法\n",
    "input = data.to(device)\n",
    "\n",
    "```\n",
    "   \n",
    "#### 书写 device-agnostic\n",
    "含义：不需要显示指定是 GPU，CPU之类，直接利用 「.to()」来执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c71e5b05d806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# at beginning of the script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# then whenever you get a new Tensor or Module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# this won't copy if they are already on the desired device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "input = data.to(device)\n",
    "model = MyModule(...).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些新的参数命名规则\n",
    "**命名规则**\n",
    " 1. name: 如果一个空字符串或包含'.'，将不再可以作为`module.add_module(name, value)`，`module.add_parameter(name, value)`或者`module.add_buffer(name, value)`的参数。\n",
    " \n",
    " **原因：**这样可能会在 `state_dict`中导致数据丢失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迁移代码对比\n",
    "- 以前写法:\n",
    "```\n",
    " model = MyRNN()\n",
    "  if use_cuda:\n",
    "      model = model.cuda()\n",
    "\n",
    "  # train\n",
    "  total_loss = 0\n",
    "  for input, target in train_loader:\n",
    "      input, target = Variable(input), Variable(target)\n",
    "      hidden = Variable(torch.zeros(*h_shape))  # init hidden\n",
    "      if use_cuda:\n",
    "          input, target, hidden = input.cuda(), target.cuda(), hidden.cuda()\n",
    "      ...  # get loss and optimize\n",
    "      total_loss += loss.data[0]\n",
    "\n",
    "  # evaluate\n",
    "  for input, target in test_loader:\n",
    "      input = Variable(input, volatile=True)\n",
    "      if use_cuda:\n",
    "          ...\n",
    "      ...\n",
    "```\n",
    "\n",
    "- 迁移后的写法：\n",
    "```\n",
    " # torch.device object used throughout this script\n",
    " use_cuda = torch.cuda.is_available() \n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  model = MyRNN().to(device)\n",
    "\n",
    "  # train\n",
    "  total_loss = 0\n",
    "  for input, target in train_loader:\n",
    "      input, target = input.to(device), target.to(device)\n",
    "      hidden = input.new_zeros(*h_shape)  # has the same device & dtype as `input`\n",
    "      ...  # get loss and optimize\n",
    "      total_loss += loss.item()           # get Python number from 1-element Tensor\n",
    "\n",
    "  # evaluate\n",
    "  with torch.no_grad():                   # operations inside don't track history\n",
    "      for input, target in test_loader:\n",
    "          ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
