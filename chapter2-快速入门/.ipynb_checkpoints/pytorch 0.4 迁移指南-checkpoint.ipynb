{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考/摘抄：https://www.cnblogs.com/z1141000271/p/9473096.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor和Variable的合并\n",
    "&emsp;&emsp;旧版本（0.1~0.3）观点认为：当前Tensor为 requires_grad = False的 Variable的特例，torch.Tensor 和 torch.autograd.Variable是同一个类，没有本质的区别.\n",
    "实际上，已经没有纯粹的Tensor，所有的Tensor对象都应支持自动求导。\n",
    "> 查看pytorch的版本类型\n",
    "```\n",
    "print(torch.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看Tensor的类型\n",
    "&emsp;&emsp; 使用「`isinstance()`」或者「`x.type()`」,但是使用python内置函数type(x)不能查看tensor变量的实际类型；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.DoubleTensor\n",
      "<class 'torch.Tensor'>\n",
      "the variable x is a torch.FloatTensor:  False\n"
     ]
    }
   ],
   "source": [
    "x = t.DoubleTensor([1, 1, 1])\n",
    "print(x.type()) # was torch.DoubleTensor\n",
    "\n",
    "print(type(x))\n",
    "print('the variable x is a torch.FloatTensor: ', isinstance(x, t.FloatTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor的requires_grad属性\n",
    "&emsp;&emsp;已经没有纯粹的tensor变量，任何一个tensor对象/变量都支持自动求导,默认求导属性为False，即 requires_grad = False;\n",
    "#### requires_grad属性使用\n",
    "- 可将 requires_grad属性作为参数，构造tensor变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(1)\n",
    "x.requires_grad  # 默认为False\n",
    "y = t.ones(1)\n",
    "z = x + y\n",
    "z.requires_grad  # 根据传导性，z的 requires_grad属性 为False\n",
    "# z.backward()     # 因为所有变量都不需要grad，因此会 Error\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum variable requires_grad and grad_fn is : \n",
      " True <ThAddBackward object at 0x7f1bc7a9a5c0>\n"
     ]
    }
   ],
   "source": [
    "# 将 requires_grad属性作为参数，构造 tensor变量\n",
    "w = t.ones(1, requires_grad = True)\n",
    "total = x + w\n",
    "print('the sum variable requires_grad and grad_fn is : \\n', total.requires_grad, total.grad_fn)\n",
    "\n",
    "# 此时可以 进行 求导，调用 backward函数\n",
    "total.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative is :  tensor([1.]) None\n",
      "tensor([1.]) None None None\n"
     ]
    }
   ],
   "source": [
    "print('the derivative is : ',w.grad,x.grad)\n",
    "\n",
    "# 因为 x,y,z 都是不需要梯度的（ requires_grad = False）,因此 x.grad,y.grad,z.grad均为None未进行计算\n",
    "print(w.grad,x.grad,y.grad,z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **`.data`**不要随意使用\n",
    "&emsp;&emsp;早期版本中，使用.data获得Variable变量中的Tensor。后期，Tensor和Variable进行了合并，因此「.data」返回一个新的requires_grad = False的Tensor！！！\n",
    "**注意：**新的Tensor同之前的Tensor变量共享内存，所以不安全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time x.requires_grad =  True\n",
      "after time x.requires_grad =  True\n",
      "after time x.requires_grad =  True\n"
     ]
    }
   ],
   "source": [
    "print('before time x.requires_grad = ', x.requires_grad)\n",
    "y = x.data # x 需要进行 autograd\n",
    "print('after time x.requires_grad = ', x.requires_grad)\n",
    "# y和x 共享内容，但是这里并不需要grad了\n",
    "# 所以会导致本来需要进行梯度的x也没有梯度可以计算了，从而x不会得到更新\n",
    "z = x.detach()\n",
    "print('after time x.requires_grad = ', x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scale 的支持\n",
    "&emsp;&emsp;早期版本中，indexing 一个一维Tensor，返回一个number类型（标量）；但是indexing 一个Variable 确实返回一个size = （1，） 的 **vector**；再比如，tensor.sum()返回一个number,但variable.sum()返回的是一个size = （1，）的vector\n",
    "** 好处** \n",
    "1. 通过引入`scalar`，可以将返回值的类型进行统一；\n",
    "2. 取得一个tensor的值（返回number），需要使用成员函数「.item()」\n",
    "3. 创建 scalar 的话， 需要使用 「torch.tensor(number)」\n",
    "4. torch.tensor（list）也可以创建 tensor\n",
    "\n",
    "**注意：** \n",
    "- tensor.sum(): tensor表示一个Tensor类型的变量\n",
    "- variable.sum(): variable表示一个Variable类型的变量\n",
    "- scalar 打印出来是没有 **[ ]**的， tensor 打印出来会用 `[]`包上\n",
    "- scalar: 0-dimension 的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(3.141592627) # 使用 torch.tensor来创建scalar\n",
    "torch.tensor(3.1415).size()  # size为 0 , was torch.Size([])\n",
    "torch.tensor([3]).size()    # compare to a vecotor of size 1\n",
    "#torch.tensor([2])          # 如果为 tensor,打印出来用 `[]`包上\n",
    "vector = torch.arange(2, 6) # this is a vector\n",
    "vector # 类似为： vector = torch.tensor([2, 3, 4, 5])\n",
    "\n",
    "# 获取 tensor中的值（vector） \n",
    "vector[3].item()   # 需要使用成员函数 「.item()」获取里面的值\n",
    "\n",
    "# 而这种 reduction 操作，返回的是一个 scalar (0-dimension 的 tensor)\n",
    "mySum = torch.tensor([2, 4, 6]).sum()\n",
    "mySum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "// TODO\n",
    "#### 累加 loss\n",
    "&emsp;&emsp;早期版本中，累加loss一般使用 「totalLoss += loss.data[0]」,后期版本累加loss，采用 「loss.item()」\n",
    "##### 为啥采用 「.data[0]」\n",
    "**原因：**loss是一个Variable， "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 弃用 volatile\n",
    "现在这个flag 已经启用了，被替换成为 torch.no_grad(), torch.set_grad_enable(grad_mode)等函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has enter the with function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    print('has enter the with function')\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has enter the with function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = x ** 2\n",
    "    print('has enter the with function')\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True) # this can also used as a funciton\n",
    "y = x ** 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes, devices 以及 numpy-style 的构造函数\n",
    "dtype 是data types，对应关系如下：\n",
    "![image](https://github.com/stonels0/pytorch-book/tree/master/chapter2-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/imgs/dtype.jpeg) \n",
    "通过**.dtype**可以得到，其他就是以前写 device.type 都是用 .cup() 或者 .cudn() ，现在独立成为一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "x = torch.randn(3, 3, dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建新的 Tensor方法\n",
    "1. 可以指定dtype 和 device 创建;\n",
    "2. 用 torch.tensor 创建 Tensor；\n",
    "3. 用 torch. * like 以及 torch.new_* 进行创建;\n",
    "&emsp;&emsp; like: 用于创建 「shape相同，数据类型相同」\n",
    "\n",
    "&emsp;&emsp; new_：用于创建属性相同，shape不想要一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9927, -1.5810, -0.1452],\n",
       "        [ 0.1451,  0.4920,  1.3241],\n",
       "        [ 0.8099, -0.6106,  0.3512]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定 dtype 和 device进行创建\n",
    "device = torch.device(\"cuda:0\")\n",
    "x = torch.randn(3, 3, dtype = torch.float64,device=device) # 当前使用device参数报错，后期研究 TODO\n",
    "# x.requires_grad # was False\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 torch.tensor创建Tesor，类似于 numpy.array()\n",
    "# 从列表中list的数据来创建\n",
    "cuda = torch.device(\"cuda\")\n",
    "torch.tensor([[1], [2], [3]], dtype = torch.half, device=cuda)\n",
    "# 创建 scalar \n",
    "torch.tensor(1)  # scalar 打印输出 不包含【】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.*like : 创建「shape、数据类型」相同\n",
    "x = torch.randn(3, dtype = torch.float64)\n",
    "torch.zeros_like(x)\n",
    "torch.zeros_like(x, dtype = torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = x.new_ones(2) # 属性相同,均为 torch.float64类型的\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 书写 device-agnostic\n",
    "含义：不需要显示指定是 GPU，CPU之类，直接利用 「.to()」来执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c71e5b05d806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# then whenever you get a new Tensor or Module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# this won't copy if they are already on the desired device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "input = data.to(device)\n",
    "model = MyModule(...).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迁移代码对比\n",
    "- 以前写法:\n",
    "```\n",
    " model = MyRNN()\n",
    "  if use_cuda:\n",
    "      model = model.cuda()\n",
    "\n",
    "  # train\n",
    "  total_loss = 0\n",
    "  for input, target in train_loader:\n",
    "      input, target = Variable(input), Variable(target)\n",
    "      hidden = Variable(torch.zeros(*h_shape))  # init hidden\n",
    "      if use_cuda:\n",
    "          input, target, hidden = input.cuda(), target.cuda(), hidden.cuda()\n",
    "      ...  # get loss and optimize\n",
    "      total_loss += loss.data[0]\n",
    "\n",
    "  # evaluate\n",
    "  for input, target in test_loader:\n",
    "      input = Variable(input, volatile=True)\n",
    "      if use_cuda:\n",
    "          ...\n",
    "      ...\n",
    "```\n",
    "\n",
    "- 迁移后的写法：\n",
    "```\n",
    " # torch.device object used throughout this script\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  model = MyRNN().to(device)\n",
    "\n",
    "  # train\n",
    "  total_loss = 0\n",
    "  for input, target in train_loader:\n",
    "      input, target = input.to(device), target.to(device)\n",
    "      hidden = input.new_zeros(*h_shape)  # has the same device & dtype as `input`\n",
    "      ...  # get loss and optimize\n",
    "      total_loss += loss.item()           # get Python number from 1-element Tensor\n",
    "\n",
    "  # evaluate\n",
    "  with torch.no_grad():                   # operations inside don't track history\n",
    "      for input, target in test_loader:\n",
    "          ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
